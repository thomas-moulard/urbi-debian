

@TechReport{	  abraham.07.seminar,
  oldkeys	= {abraham.07.seminar.conceptgcc},
  author	= {Alexandre Abraham},
  title		= {{ConceptC++} study and possible integration in {SCOOP}},
  titre		= {Etude de {ConceptC++} et possible int\'egration dans
		  {SCOOP}},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2007,
  url		= {http://publications.lrde.epita.fr/200706-Seminar-Abraham},
  urllrde	= {200706-Seminar-Abraham},
  abstract	= {At the end of this decade will arise C++0x and with it the
		  new ``concept'' paradigm. Concepts provide abstract types
		  as well as all the equipment to adapt concrete types to
		  their abstraction, just like \emph{Static} library, a part
		  of Olena project, does.\\
		  
		  We compare these two approaches, highlighting their
		  respective strengths and weaknesses, in order to lay the
		  foundations for the integration of concepts into SCOOP. In
		  fact, concepts will simplify the client code and bring some
		  new features to SCOOP.},
  resume	= {La fin de cette d\'ecennie verra l'av\`enement de C++0x et
		  avec lui du nouveau paradigme de \og concepts \fg{}. Les
		  concepts fournissent un m\'ecanisme de typage abstrait pour
		  les types param\'etr\'es ainsi que tout l'\'equipement
		  d'adaptation des types concrets \`a ces types abstraits
		  comme le fait actuellement la biblioth\`eque \emph{Static},
		  composant du projet Olena.\\
		  
		  Nous proposons donc un comparatif de ces approches en
		  exhibant leurs points forts et faibles ainsi que leurs
		  capacit\'es particuli\`eres afin de proposer un support de
		  documentation et une base pour la future int\'egration des
		  concepts dans le paradigme SCOOP. Les concepts
		  simplifieront l'\'ecriture du code client et enrichiront
		  SCOOP de fonctions suppl\'ementaires.}
}

@TechReport{	  abraham.08.seminar,
  author	= {Alexandre Abraham},
  title		= {Topological Watershed},
  titre		= {Ligne de partage des eaux topologique},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2008,
  url		= {http://publications.lrde.epita.fr/200806-Seminar-Abraham},
  urllrde	= {200806-Seminar-Abraham},
  abstract	= {Dividing a picture into area of interest is called picture
		  segmentation, it is useful in particular to point out
		  cancerous cells in medical imaging. The \emph{Watershed
		  Transform} provides such a segementation and can be
		  implemented in many ways. Here we will focus on the \emph{
		  Topological Watershed}, a performant algorithm producing
		  results with nice properties. In this report, we will show
		  how this algorithm had been implemented in Milena, the C++
		  generic image processing library of Olena, developed at the
		  LRDE. We will first see how to treat usual image format and
		  then generalize it to trickier formats like pictures mapped
		  on general graphs.},
  resume	= {Segmenter une image consiste \`a en extraire les r\'egions
		  d'int\'er\^et, par exemple pour s\'eparer des cellules
		  canc\'ereuses en imagerie m\'edicale. L'approche par
		  transformation de la ligne de partage des eaux (LPE) ou
		  \emph{Watershed Transform} permet d'obtenir une telle
		  segmentation. Il en existe de nombreuses d\'efinitions,
		  ainsi que diverses impl\'ementations, dont certaines sont
		  \`a la fois performantes et produisent un r\'esultat avec
		  de bonnes propri\'et\'es, comme le \emph{Topological
		  Watershed}. Cet expos\'e pr\'esentera l'impl\'ementation
		  d'un algorithme calculant cette LPE au sein de Milena, la
		  biblioth\`eque C++ g\'en\'erique de traitement d'image de
		  la plate-forme Olena, d\'evelopp\'ee au LRDE. Nous nous
		  int\'eresserons tout d'abord aux formats d'images
		  ``classiques'', puis \`a la g\'en\'eralisation \`a des
		  formats d'images plus inhabituels (images \`a support de graphe g\'en\'eraux, etc.).}
}

@TechReport{	  abraham.09.seminar,
  author	= {Alexandre Abraham},
  title		= {Morphology on color images},
  titre		= {Morphologie sur images couleur},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2009,
  url		= {http://publications.lrde.epita.fr/200901-Seminar-Abraham},
  urllrde	= {200901-Seminar-Abraham},
  abstract	= {This work takes place in the context of Milena, the C++
		  generic image processing library of the Olena platform,
		  developed at LRDE. Morphological algorithms are one of
		  Milena's major assets. Yet few image processing libraries
		  implement them even though they are very useful. Those
		  algorithms require supremum and infimum operators, which
		  don't exist by default for composite types like
		  red-green-blue (RGB) pixels. We therefore propose the
		  implementation of those operators for RGB values, along
		  with a complete toolchain allowing morphological algorithms
		  to work on color images.},
  resume	= {Les algorithmes morphologiques sont l'un des atouts
		  majeurs de Milena, la biblioth\`eque de traitement d'image
		  g\'en\'erique et performante d\'evelopp\'ee au LRDE. En
		  effet, ils sont tr\`es utiles et relativement peu
		  impl\'ement\'es dans les autres biblioth\`eques. Ces
		  algorithmes requi\`erent des op\'erateurs de bornes
		  sup\'erieure et inf\'erieure (\emph{supremum} et
		  \emph{infimum}) qui n'existent pas par d\'efaut pour des
		  types composites comme les couleurs encod\'ees en
		  rouge-vert-bleu (RVB). Nous pr\'esentons donc une
		  impl\'ementation de ces op\'erateurs pour le type RVB ainsi
		  que toute la cha\^ine de traitement permettant de faire
		  fonctionner des algorithmes morphologiques sur des images en couleurs.}
}

@TechReport{	  anisko.03.seminar,
  oldkeys	= {trans-tech-rep,anisko.03},
  title		= {Transformers: a {C++} program transformation framework},
  author	= {Robert Anisko and Valentin David and Cl\'ement Vasseur},
  institution	= {LRDE},
  year		= 2003,
  number	= 0310,
  urllrde	= {20030521-Seminar-ClementVasseur-Transformers-Report}
}

@TechReport{	  ballas.07.seminar,
  oldkeys	= {ballas.07.seminar.olena.core},
  author	= {Nicolas Ballas},
  title		= {Software engineering in {O}lena {C}ore},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2007,
  url		= {http://publications.lrde.epita.fr/200706-Seminar-Ballas},
  urllrde	= {200706-Seminar-Ballas},
  abstract	= {Software engineering defines some methods in order to
		  guarantee software quality. In the image processing field,
		  several image types exist. Also, this is difficult to build
		  a library dedicated to this field which provides reusable,
		  extensible or compatible tools. We will see different
		  approaches used by generic image processing libraries which
		  deal with this problem.}
}

@TechReport{	  ballas.08.seminar,
  author	= {Nicolas Ballas},
  title		= {Image taxonomy in {M}ilena},
  titre		= {Taxonomie des images de {M}ilena},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2008,
  url		= {http://publications.lrde.epita.fr/200806-Seminar-Ballas},
  urllrde	= {200806-Seminar-Ballas},
  abstract	= {Milena is the generic image processing library of the
		  Olena platform. The library aims at remaining simple while
		  providing high performances. The introduction of new image
		  types based on graphs has revealed some design problems
		  limit ing its genericity. For instance, we have always
		  considered that "images have points"; yet some images have
		  sites that are not points (but edges, facets, and even sets
		  of points). Another erroneous assumption was to consider
		  that sites are localized by a vector (e.g., (x,y) in the 2D
		  plane), which cannot be true when sites are not point-wise.
		  Therefore there was a need to reconsider the image types
		  and their underlying images properties.In this seminar, we
		  will present a new image taxonomy that solves those
		  issues.},
  resume	= {Milena est la biblioth\`eque de traitement d'image
		  g\'en\'erique de la plate-forme Olena. Cette biblioth\`eque
		  a pour but d'\^etre performante tout en restant simple.
		  L'introduction dans Milena de nouveaux types d'images
		  bas\'es sur des graphes a mis en \'evidence des probl\`emes
		  de mod\'elisation qui sont un frein pour sa
		  g\'en\'ericit\'e. Par exemple, nous avons toujours
		  consid\'er\'e que "les images ont des points". N\'eanmoins,
		  certains types d'images poss\`edent des sites qui ne sont
		  pas des points (mais des arr\^etes, faces, ou m\^eme des
		  ensembles de points). Une autre supposition erron\'ee
		  \'etait de consid\'erer que les sites \'etaient toujours
		  localis\'es par un vecteur (c\`ad, (x,y) dans le plan 2D).
		  Cette supposition est fausse lorsque l'on manipule des
		  sites qui ne sont pas "Pointwise". Il etait donc
		  n\'ecessaire de modifier les types d'images utilis\'es dans
		  Milena et les propri\'et\'es qui leur sont associ\'ees.
		  Pendant ce s\'eminaire, nous pr\'esenterons une nouvelle
		  classification d'images permettant de r\'esoudre ces probl\`emes.}
}

@TechReport{	  ballas.09.seminar,
  author	= {Nicolas Ballas},
  title		= {Properties in {M}ilena},
  titre		= {Les propri\'et\'es dans {M}ilena},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2009,
  url		= {http://publications.lrde.epita.fr/200901-Seminar-Ballas},
  urllrde	= {200901-Seminar-Ballas},
  abstract	= {Getting both high performance and genericity at the same
		  time is one of the major research field at the LRDE.
		  Milena, the Olena platform library, faces that issue in the
		  context of image processing. Furthermore Milena has for
		  extra objective to remain simple to use for a practitioner.
		  A solution, experimented since several years, is based on
		  properties. Properties are a set of features statically
		  bound to a type. For instance, image types in Milena have a
		  property named speed that gives information at compile time
		  about the value access time. In this seminar, we focus on
		  the image properties. We detail the definition of those
		  properties and justify them. We show how those properties
		  help in improving efficiency while maintaining genericity.
		  For that, we take as illustration the implementation of low
		  level routines in the library.},
  resume	= {Avoir de hautes performances tout en conservant la
		  g\'en\'ericit\'e est un des domaines de recherche
		  pr\'epond\'erant au sein du LRDE. Milena, la biblioth\`eque
		  de la plate-forme Olena, confronte ce probl\`eme au domaine
		  du traitement d'image. De plus, Milena a aussi pour
		  objectif de rester simple \`a utiliser. Une solution \`a
		  ces probl\`emes, utilis\'ee depuis plusieurs ann\'ees,
		  repose sur les propri\'et\'es. Les propri\'et\'es sont un
		  ensemble de caract\'eristiques associ\'ees statiquement \`a
		  un type particulier. Par exemple, les types d'images de
		  Milena poss\`edent une propri\'et\'e speed qui indique les
		  temps d'acc\`es aux valeurs des images. Durant ce
		  s\'eminaire, nous nous int\'eresserons aux propri\'et\'es
		  des types d'images. Nous d\'etaillerons les d\'efinitions
		  de ces propri\'et\'ees. Nous montrerons aussi comment les
		  propri\'et\'es aident \`a am\'eliorer les performances tout
		  en maintenant la g\'en\'ericit\'e. Pour cela, nous
		  prendrons en illustration l'impl\'ementation des routines bas niveau dans la biblioth\`eque.}
}

@TechReport{	  berger.05.seminar,
  author	= {Christophe Berger and Nicolas Widynski},
  title		= {Using connected operators to manipulate image components},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  urllrde	= {200507-Seminar-Berger-Widynski},
  year		= 2005,
  abstract	= {Connected operators are morphological filters which have
		  the property of keeping objects contours when simplifying
		  images. They bring to the light objects situated in the
		  image. To do it, an implementation of the Tarjan's
		  Union-find algorithm is used for an easy manipulation of
		  image components. A binary partition tree is built, in
		  order to simplify the objects attributes computation and
		  the filtering of image. First of all, we will introduce
		  morphological filters and connected operators, then we will
		  propose an overview of different kinds of methods used in
		  the literature in order to create a binary partition tree
		  and we will explain the Tarjan's "union-find" algorithm for
		  the image filtering. At last, we will apply this method in
		  order to clean and delete stars in space's images.}
}

@TechReport{	  berger.06.seminar,
  oldkeys	= {berger.06.seminar.taxonomy},
  author	= {Christophe Berger},
  title		= {Image taxonomy in {O}lena},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  urllrde	= {200605-Seminar-Berger},
  year		= 2006
}

@TechReport{	  bigaignon.05.seminar,
  oldkeys	= {bigaignon.aut-to-exp.05.seminar},
  author	= {Robert Bigaignon},
  title		= {Computing the regular language recognized by a finite
		  automaton},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2005,
  urllrde	= {200506-Seminar-Bigaignon}
}

@TechReport{	  cadilhac.05.seminar,
  oldkeys	= {cadilhac.cover-automata.05.seminar},
  author	= {Micha\"el Cadilhac},
  title		= {Cover automata for finite languages},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2005,
  urllrde	= {20050622-Seminar-Cadilhac-CoverAutomata-Report}
}

@TechReport{	  carlinet.09.seminar,
  author	= {Edwin Carlinet},
  titre		= {Les arbres de composantes dans Milena},
  title		= {Component trees in Milena},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2009,
  urllrde	= {200906-Seminar-Carlinet},
  url		= {http://publications.lrde.epita.fr/200906-Seminar-Carlinet}
		  ,
  abstract	= {Pattern recognition and object detection are very
		  important stakes in image processing. Many solutions have
		  been provided; nevertheless the one based on component
		  trees seems to be the most promising. A component tree
		  allows to work on connected components at different
		  gray-levels in the image. Thanks to this tree, we can use
		  attributes as criteria to identify components, such a
		  component being an object in the image. During this
		  seminar, we present a component tree implementation, how to
		  deal with attributes, different methods dedicated to
		  component identification and a general processing chain
		  that leads to object recognition. },
  resume	= {La d\'etection des formes et la reconnaissance d'objets
		  font parties des enjeux les plus importants du traitement
		  d'images. Diff\'erentes strat\'egies ont d\'ej\`a vu le
		  jour; n\'eanmoins celle bas\'ee sur l'utilisation des
		  arbres de composantes semble particuli\`erement
		  prometteuse. En effet, l'arbre de composantes permet
		  d'\'etablir puis de mettre en relation les composantes \`a
		  diff\'erents niveaux de gris de l'image. A partir de cette
		  arbre, il devient alors possible d'appliquer des attributs
		  qui serviront de crit\`eres pour filtrer ces composantes et
		  mettre en \'evidence les objets de l'image. Nous
		  pr\'esenterons donc l'impl\'ementation de ces arbres,
		  l'utilisation des attributs et des politiques de
		  propagation pour filtrer les composantes, ainsi que la
		  cha\^ine de traitement qui permettra d'identifier ces objets. }
}

@TechReport{	  charron.08.seminar,
  author	= {Samuel Charron},
  title		= {Homolib},
  titre		= {Homolib},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2008,
  url		= {http://publications.lrde.epita.fr/20080901-Seminar-Charron}
		  ,
  urllrde	= {20080901-Seminar-Charron},
  abstract	= {Decision Diagrams are a family of data structures that
		  represents huge data sets using a small amount of memory.
		  These structures can be of fixed size (tuples), or varying
		  size (lists, maps, ...), the DD handling being different
		  for each one. Data Decision Diagrams and Set Decision
		  Diagrams handle varying size data thanks to operations
		  named homomorphisms. However, the definition of a correct
		  operation can be hard because numerous errors hard to
		  identify can happen. This seminar offers a presentation of
		  an algorithms library that gives a more abstract view on
		  handled data. This library contains the algorithms defined
		  in "List" and "Map" modules from the Objective Caml
		  standard library, allowing the user to focus on his
		  specific problem.},
  resume	= {Les Diagrammes de D\'ecision sont une famille de
		  structures de donn\'ees permettant de repr\'esenter avec
		  peu de m\'emoire de grands ensembles de donn\'ees. Ces
		  structures peuvent \^etre de taille fixe (un tuple) ou
		  variable (une liste, un conteneur associatif, \ldots), la
		  manipulation du DD ne se faisant pas de la m\^eme
		  mani\`ere. Les Data Decision Diagrams et Set Decision
		  Diagrams manipulent des donn\'ees de taille variable
		  gr\^ace \`a des op\'erations, les homomorphismes. Cependant
		  la d\'efinition d'une op\'eration correcte peut d\'erouter
		  l'utilisateur, et passe souvent par de nombreuses erreurs,
		  difficiles identifier. Ce s\'eminaire propose une
		  biblioth\`eque d\`ualgorithmes fournissant une vue plus
		  abstraite que les homomorphismes "bruts" des donn\'ees
		  manipul\'ees, en reprenant les algorithmes d\'efinis dans
		  les modules "List" et "Map" d'Objective Caml. L'utilisateur
		  peut se concentrer sur les parties sp\'ecifiques \`a son probl\`eme.}
}

@TechReport{	  claveirole.04.seminar.analysis,
  oldkeys	= {claveirole.vcsn-analysis.04},
  author	= {Thomas Claveirole},
  title		= {Analysis of the {V}aucanson project},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2004,
  urllrde	= {20040526-Seminar-Claveirole-Vaucanson_Analysis-Slides}
}

@TechReport{	  claveirole.04.seminar.overview,
  oldkeys	= {claveirole.vcsn-overview.04},
  author	= {Thomas Claveirole},
  title		= {An overview of {V}aucanson},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2004,
  urllrde	= {20041124-Seminar-Claveirole-VaucansonOverview-Report}
}

@TechReport{	  d-halluin.08.seminar,
  author	= {Florent D'Halluin},
  title		= {{Y}et {A}nother {V}aucanson {GUI}},
  titre		= {Interface graphique de {V}aucanson},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2008,
  abstract	= {Vaucanson is a finite state machine manipulation platform.
		  Since it was started in 2002, the project has been
		  attracting more and more users. In that regard, it requires
		  an efficient user front end.\par For non-expert users,
		  automaton manipulation can be done via taf-kit, a set of
		  command-line tools. A first GUI was developed in 2005. Its
		  use was slow and complicated since it relied on taf-kit for
		  every operation.\par This new GUI, plugged directly into
		  the core of the Vaucanson library for efficiency,
		  simplifies the automaton manipulation process and makes
		  full use of the generic algorithms included in the library.},
  resume	= {Vaucanson est une plateforme de manipulation d'automates
		  finis. D\'ebut\'e en 2002, le projet attire de plus en plus
		  d'utilisateurs. De ce fait, une interface utilisateur
		  efficace est n\'ecessaire.\par Pour l'utilisateur non
		  expert, la manipulation d'automates peut s'effectuer via
		  taf-kit, une suite d'outils accessible en ligne de
		  commande. Une premi\`ere interface graphique avait \'et\'e
		  esquiss\'ee en 2005, mais son fonctionnement \'etait lent
		  et compliqu\'e car elle s'appuyait sur taf-kit pour
		  r\'ealiser chaque op\'eration.\par Cette nouvelle interface
		  graphique, branch\'ee directement sur le c\oe{}ur de la
		  biblioth\`eque pour plus d'efficacit\'e, simplifie la
		  manipulation d'automates et rend accessible les algorithmes
		  g\'en\'eriques de Vaucanson.},
  url		= {http://publis.lrde.epita.fr/200807-Seminar-DHalluin},
  urllrde	= {200807-Seminar-DHalluin}
}

@TechReport{	  d-halluin.09.seminar,
  author	= {Florent D'Halluin},
  title		= {Performance analysis in large C++ project},
  titre		= {Analyse de performances pour projets C++},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2009,
  abstract	= {Vaucanson is an extensive C++ library for the manipulation
		  of finite state machines. Compared to its main competitor,
		  OpenFST, Vaucanson has major performance issues. In order
		  to improve the performance of Vaucanson, a set of tools is
		  required to analyse the library's behavior in terms of CPU
		  time requirements and memory usage. Up to March 2009, no
		  such tools existed that could be of practical use with
		  Vaucanson. CBS is a C++ Benchmarking Suite that measures
		  the performance of C++ projects and provides tools to
		  display, analyse and compare results in a human-readable
		  form. It is used for in-depth Vaucanson profiling, and
		  helps rewrite algorithms.},
  resume	= {Vaucanson est une biblioth\`eque C++ de manipulation
		  d'automates finis. Par rapport \`a son concurrent
		  principal, OpenFST, Vaucanson souffre d'importants
		  probl\`emes de performances. Afin d'am\'eliorer les
		  performances de Vaucanson, il est n\'ecessaire d'avoir des
		  outils appropri\'es pour analyser le comportement de la
		  biblioth\`eque en termes d'utilisation de temps CPU et de
		  gestion de la m\'emoire. Jusqu'en Mars 2009, il n'existait
		  pas d'outils de ce type pratiques \`a utiliser avec
		  Vaucanson. CBS (C++ Benchmarking Suite) est une suite
		  d'outils d'analyse de performances pour projets C++. Ces
		  outils permettent de mesurer, d'afficher, et de comparer
		  l'utilisation de ressources (temps, m\'emoire), dans un
		  format accessible \`a l'utilisateur. Ils sont utilis\'es
		  pour analyser Vaucanson afin de r\'e\'ecrire les
		  algorithmes les moins efficaces.},
  url		= {http://publis.lrde.epita.fr/200905-Seminar-DHalluin},
  urllrde	= {200905-Seminar-DHalluin}
}

@TechReport{	  damota.09.seminar,
  author	= {Samuel Da Mota},
  title		= {Nondeterminisation of alternating automata in SPOT},
  titre		= {Nond\'eterminisation d'automates alternants dans SPOT},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2009,
  abstract	= {SPOT is a C++ model checking library that relies on the
		  automata theoretic approach to model checking. The first
		  step of this approach consists in the translation of an LTL
		  formula to an automaton that recognizes the same language.
		  Currently, there exists two algorithms in SPOT that
		  translate LTL formul\ae{} to transition based generalized
		  B\"uchi automata (TGBA). We want to implement a third
		  translation for comparison. This new method translates the
		  LTL formula into an alternating automaton, and then applies
		  a nondeterminisation algorithm which takes as input the
		  alternating automaton and gives out a TGBA. \\ To complete
		  this task, the alternating automaton structure will first
		  be set up in SPOT, then the nondeterminization algorithm
		  will be implemented, and last, the translation of LTL
		  formul\ae{} to alternating automata will be added.},
  resume	= {SPOT est une biblioth\`eque de \emph{model checking}
		  bas\'ee sur l'approche par automates et sur les automates
		  de B\"uchi g\'en\'eralis\'e bas\'e sur les transitions
		  (TGBA) pour la v\'erification de syst\`emes exprim\'es sous
		  forme de formules logiques. L'approche automate consite \`a
		  traduire une formule logique LTL en un automate qui
		  reconnait le m\^eme language. Dans l'\'etat actuel des
		  choses, il existe deux algorithmes dans SPOT de traduction
		  de formules LTL vers des TGBA. Un troisi\`eme algorithme y
		  sera rajout\'e. Ce dernier convertira dans un premier temps
		  la formule LTL en un automate alternant, puis appliquera un
		  algorithme de nond\'eterminisation sur celui-ci pour
		  obtenir un TGBA. Le but de cette nouvelle impl\'ementation
		  est de mener une comparaison avec les deux autres d\'ej\`a
		  en place. \\ Pour mener \`a bien cette t\^ache, la
		  structure des automates altenants sera int\'egr\'ee dans
		  SPOT, puis ce sera au tour de l'algorithme de
		  nond\'eterminisation d'automates alternants vers des TGBA
		  et enfin la traduction de formules LTL sous forme d'automates alternants.},
  url		= {http://publis.lrde.epita.fr/200906-Seminar-DaMota},
  urllrde	= {200906-Seminar-DaMota}
}

@TechReport{	  david.04.seminar,
  oldkeys	= {attr-tech-rep},
  title		= {Attribute grammars for {C++} disambiguation},
  author	= {Valentin David},
  institution	= {LRDE},
  year		= 2004,
  urllrde	= {20041201-Seminar-David-Attribute-Report}
}

@TechReport{	  deledalle.07.seminar,
  author	= {Charles-Alban Deledalle},
  title		= {Factor analysis based channel compensation in speaker
		  verification},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2007,
  url		= {http://publications.lrde.epita.fr/200705-Seminar-Deledalle}
		  ,
  urllrde	= {200705-Seminar-Deledalle}
}

@TechReport{	  deledalle.08.seminar,
  author	= {Charles-Alban Deledalle},
  title		= {{SVM} Kernel Combining System for Speaker Verification},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2008,
  url		= {http://publications.lrde.epita.fr/200801-Seminar-Deledalle}
		  ,
  urllrde	= {200801-Seminar-Deledalle},
  abstract	= {The best speaker verification systems are based on score
		  combination of several approaches. Support Vector Machines
		  (SVM) give very hopeful results. Thus, combining these
		  methods could be very efficient. In our approach, we
		  propose a new combination method for speaker verification
		  systems based on SVM methods. This one performs a linear
		  combination of several kernel functions in order to produce
		  a new kernel function. In this combination, the weights are
		  speaker dependent, by opposition of score combination
		  approach for which the weight are universal. The idea is to
		  adapt the combination weights for each speaker in order to
		  take the advantage of the best kernel. In our experiment,
		  combinations are performed on several kernel functions: the
		  GLDS kernel, linear and Gaussian GMM supervector kernels.
		  The method can use every kernel functions with no
		  modification. The experiments are done on the NIST-SRE 2005
		  and 2006 (all trials) database.},
  resume	= {Les meilleurs syst\`emes de Verification du Locuteur (VL)
		  sont fond\'es sur la fusion des scores de d\'ecision de
		  plusieurs approches. Les m\'ethodes bas\'ees sur les
		  S\'eparateurs \`a Vaste Marge (SVM) donnent des r\'esultats
		  tr\`es performants. En cons\'equence, l'apport de ces
		  m\'ethodes est tr\`es important pour la fusion. Dans notre
		  approche, nous proposons une nouvelle m\'ethode de fusion
		  des syst\`emes de VL bas\'es sur les m\'ethodes SVM en
		  construisant une nouvelle fonction noyau \`a partir d'une
		  combinaison lin\'eaire de plusieurs fonctions. Dans cette
		  combinaison, les poids utilis\'es varient selon les
		  locuteurs, ce qui diff\`ere des approches par fusion de
		  score qui elles utilisent des poids universels. L'id\'ee
		  est donc de tirer avantage des performances de chacun des
		  noyaux, et cela pour chaque locuteur donn\'e. Ces
		  combinaisons sont effectu\'ees sur plusieurs types de
		  noyaux dont les noyaux GLDS, GMM supervecteurs lin\'eaires
		  et Gaussiens. Les exp\'eriences sont r\'ealis\'ees sur la
		  base des corpus NIST-SRE 2005 et 2006.}
}

@TechReport{	  delmon.07.seminar,
  oldkeys	= {delmon.07.seminar.eps.removal},
  author	= {Vivien Delmon},
  title		= {Generic epsilon-removal},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2007,
  url		= {http://publications.lrde.epita.fr/200706-Seminar-Delmon},
  urllrde	= {200706-Seminar-Delmon}
}

@TechReport{	  delmon.08.seminar,
  author	= {Vivien Delmon},
  title		= {Rational Expression Parser},
  titre		= {Parser d'expressions rationnelles},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2008,
  url		= {http://publications.lrde.epita.fr/200806-Seminar-Delmon},
  urllrde	= {200806-Seminar-Delmon},
  abstract	= {The Vaucanson library is designed to manipulate automata
		  and transducers. Therefore we need a rational expression
		  parser which deals with transducers. The current rational
		  expression parser only takes as input weighted rational
		  expression. The new parser allows us to specify any kind of
		  weight and any kind of monoid like free monoid product.
		  Both of these features are mandatory if we want to deal
		  with transducers. The new parser is also less restrictive
		  and provides more freedom to the user who can easily change
		  the form of the grammar used to write its expression.},
  resume	= {La biblioth\`eque Vaucanson permet de manipuler des
		  automates et des transducteurs. Le parser d'expression
		  rationnelles doit donc lui aussi traiter ces diff\'erentes
		  structures. Malheureusement l'ancien parser ne permettait
		  pas de lire des expressions rationnelles d\'ecrivant des
		  transducteurs ou m\^eme des automates \`a poids autres que
		  des nombres. Le nouveau parser permet de lire des
		  expressions rationnelles contenant des poids de toutes
		  sortes et des alphabets d\'efinis sur des produits de
		  mono\"ides. Ces diff\'erentes am\'eliorations permettent
		  d'interpr\'eter des expressions rationnelles complexes
		  repr\'esentant entre autres des transducteurs. }
}

@TechReport{	  delmon.08.seminar.reduce,
  author	= {Vivien Delmon},
  title		= {Automata Reduction},
  titre		= {Reduction d'automates},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2008,
  url		= {http://publications.lrde.epita.fr/200901-Seminar-Delmon},
  urllrde	= {200901-Seminar-Delmon},
  abstract	= { A deterministic automaton can be minimized efficiently
		  into an automaton that has a minimal number of states. The
		  reduction algorithm presented here produces an automaton
		  with a minimal number of states from a non deterministic
		  automaton with weights defined in a field. This algorithm
		  computes the base of the vector space generated by the
		  series represented by the automaton and produces an
		  automaton with a number of states equal to the dimension of
		  this base. To find this base the algorithm has to solve a
		  system of linear equations, this step requires the semiring
		  to be a field. We also want to run our algorithm on fields
		  that are not commutative which forbids the use of classical
		  solvers for these systems. This report shows how we deal
		  with these different constraints. We also present a
		  modified algorithm to work with series over Z semiring
		  which is not a field but has some sufficient properties. },
  resume	= {Un automate d\`eterministe peut \^etre minimis\`e de
		  mani\`ere efficace et donne un automate dont le nombre
		  d'\'etats est minimal. L'algorithme de r\'eduction
		  pr\'esent\'e dans ce rapport permet de construire un
		  automate dont le nombre d'\'etats est minimal \`a partir
		  d'un automate non d\'eterministe dont les poids sont
		  d\'efinis sur un semi-anneau qui est un corps. L'algorithme
		  calcule pour cela la base de l'espace vectoriel engendr\'e
		  par la s\'erie repr\'esent\'e par l'automate, ce qui permet
		  de construire un automate dont le nombre d'\'etats est
		  \'egal \`a la dimension de cette base. L'algorithme se base
		  sur la repr\'esentation matricielle des automates et nous
		  am\`ene \`a r\'esoudre un syst\`eme d'\'equations
		  lin\'eaires, ce qui force le semi-anneau de notre s\'erie
		  \`a avoir les propri\'et\'es d'un corps. Nous voulons aussi
		  que notre algorithme fonctionne sur des s\'eries dont le
		  semi-anneau est un corps non commutatif ce qui nous
		  emp\^eche d'utiliser les techniques classiques de
		  r\'esolution de syst\`emes lin\'eaires. Ce rapport montre
		  comment passer outre ces difficult\'es. Nous verrons enfin
		  comment adapter notre algorithme pour qu'il fonctionne sur
		  Z qui n'est pas un corps mais qui poss\`ede des propri\'et\'es suffisantes. }
}

@TechReport{	  denuziere.09.seminar,
  author	= { Lo\"ic Denuzi\`ere },
  title		= {{CLIMB}: A Dynamic Approach To Generic Image Processing},
  titre		= { {CLIMB}: Une approche dynamique du traitement
		  g\'en\'erique d'images },
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2009,
  url		= {http://publications.lrde.epita.fr/200905-Seminar-Denuziere}
		  ,
  urllrde	= {200801-Seminar-Denuziere},
  abstract	= { Olena is one of the most advanced image processing
		  libraries in terms of genericity. Its fully static design
		  allows for high performance, although sometimes at the cost
		  of overweighted syntax and longer compilation times. This
		  makes it less convenient for incremental development,
		  experimentation and rapid prototyping. We will present a
		  different approach to generic image processing which, while
		  using the same domain model as Olena, focuses on dynamicity
		  aspects and offers a totally different use of the library.
		  The cornerstone is the Common Lisp programming language
		  which opens a perspective for interactive use, on-the-fly
		  creation of image types and algorithms as well as a clear,
		  customizable and extensible syntax for common operations.
		  },
  resume	= { Olena est l'une des biblioth\`eques de traitement
		  d'images dont la g\'en\'ericit\'e est la plus pouss\'ee.
		  Son mod\`ele enti\`erement statique permet de tr\`es bonnes
		  performances, au prix d'une syntaxe alourdie et de temps de
		  compilation importants. Ces aspects la rendent moins
		  efficace pour le d\'eveloppement incr\'emen\-tal,
		  l'exp\'erimentation et le prototypage rapide. Nous
		  pr\'esenterons une approche diff\'erente du traitement
		  d'images g\'en\'erique qui utilise la m\^eme mod\'elisation
		  du domaine qu'Olena mais se concentre sur les aspects
		  dynamiques, offrant ainsi une utilisation totalement
		  diff\'erente de la biblioth\`eque. La pierre angulaire est
		  le langage Common Lisp qui permet une utilisation
		  interactive, la cr\'eation \`a la vol\'ee de nouveaux types
		  d'images ou de nouveaux algorithmes, tout en offrant une
		  syntaxe claire, personnalisable et extensible pour les op\'erations courantes. }
}

@TechReport{	  depres.05.seminar,
  oldkeys	= {depres.reg-bench.05.seminar},
  author	= { Nicolas Despres },
  title		= { Regression benchmarking },
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2005,
  urllrde	= {200506-Seminar-Despres}
}

@TechReport{	  duhamel.08.seminar,
  author	= {Guillaume Duhamel},
  title		= {Stage de traitement d'image au {LRDE}},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2008,
  url		= {http://publications.lrde.epita.fr/200801-Seminar-Duhamel},
  urllrde	= {200801-Seminar-Duhamel}
}

@TechReport{	  durlin.07.seminar,
  oldkeys	= {durlin.07.seminar.disambiguation},
  author	= {Renaud Durlin},
  title		= {Semantics driven disambiguation},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2007,
  url		= {http://publications.lrde.epita.fr/200706-Seminar-Durlin},
  urllrde	= {200706-Seminar-Durlin},
  abstract	= {An elegant approach to manage ambiguous grammars consists
		  in using a generalized LR parser which will not produce a
		  parse tree but a parse forest. An additional step, called
		  disambiguation, occurring just after the parsing, is then
		  necessary. The disambiguation process consists in analyzing
		  the parse forest to choose the only good parse tree using
		  semantics rules. We use this approach in Transformers with
		  the attribute grammars formalism. The lab work will be a
		  comparison between this formalism and two other methods of
		  disambiguation: the first one using ASF+SDF and the second
		  one using Stratego language. The goal of this comparison
		  will try to emphasize that attribute grammars are perfect
		  to solve the disambiguation problem. Another thing will be
		  to find the weakness of this method compared to the two
		  others for a possible improvement of the system used in
		  Transformers.},
  resume	= {Une approche \'el\'egante pour g\'erer les grammaires
		  ambigu\"es consiste \`a utiliser un parseur LR
		  g\'en\'eralis\'e qui produira non pas un arbre mais une
		  for\^et de parse. Une \'etape suppl\'ementaire, appel\'ee
		  d\'esambiguisation, survenant juste apr\`es le parsing, est
		  alors n\'ecessaire. Celle-ci consiste analyser cette
		  for\^et pour obtenir l'unique arbre valide correspondant
		  \`a l'entr\'ee en prenant en compte les r\`egles de
		  s\'emantiques contextuelles. C'est cette approche qui a
		  \'et retenue dans Transformers avec le formalisme des
		  grammaires attribu\'ees. Le travail effectu\'e pr\'esentera
		  une comparaison entre ce formalisme et deux autres
		  techniques de d\'esambiguisation : la premi\`ere \`a l'aide
		  d'ASF+SDF et la deuxi\`eme \`a l'aide du langage Stratego.
		  Le but de cette comparaison sera double : montrer que les
		  grammaires attribu\'ees sont parfaitement adapt\'ees \`a ce
		  probl\`eme et exhiber les faiblesses de celles-ci par
		  rapport aux deux autres m\'ethodes en vue d'une
		  am\'elioration possible du syst\`eme utilis\'e dans Transformers.}
}

@TechReport{	  durlin.08.seminar,
  author	= {Renaud Durlin},
  title		= {Semantics driven disambiguation: A comparison of different
		  approaches},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2008,
  url		= {http://publications.lrde.epita.fr/200801-Seminar-Durlin},
  urllrde	= {200801-Seminar-Durlin},
  abstract	= {Modularity, scalability and expressiveness, three main
		  aspects for a disambiguation system. Disambiguation is the
		  step occurring just after the parsing that consists in
		  analyzing the output given by a generalized LR parser. The
		  goal is to choose, amongst the many parse trees, the right
		  one that corresponds to the input using semantics rules. By
		  means of a comparison with two other methods based on SDF
		  (the first one using ASF formalism and the second one using
		  Stratego language), our approach, attribute grammars, will
		  be evaluated with respect to these three aspects to bring
		  out its strengths and its weaknesses.},
  resume	= {Modularit\'e, extensibilit\'e et expressivit\'e, trois
		  aspects fondamentaux pour un syst\`eme de
		  d\'esambigu\"isation. La d\'esambigu\"isation est l'\'etape
		  survenant juste apr\`es l'analyse syntaxique qui consiste
		  \`a analyser la sortie obtenue lors de l'utilisation d'un
		  parseur LR g\'en\'eralis\'e. Le but de cette \'etape
		  \'etant de s\'electionner, parmi toute une for\^et,
		  l'unique arbre valide correspondant \`a l'entr\'ee en
		  prenant en compte les r\`egles de s\'emantique
		  contextuelles. Au travers d'une comparaison avec deux
		  autres techniques reposant sur SDF (le formalisme ASF et le
		  langage Stratego), le syst\`eme de grammaires attribu\'ees
		  utilis\'e dans Transformers sera \'evalu par rapport \`a
		  ces aspect fondamentaux pour en faire ressortir les avantages et inconv\'enients.}
}

@TechReport{	  folio.08.seminar,
  author	= {Etienne Folio},
  title		= {Distance Transform},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2008,
  urllrde	= {200807-Seminar-Folio},
  abstract	= {A distance transform, also known as distance map or
		  distance field, is a representation of a distance function
		  to an object, as an image. Such maps are used in several
		  applications, especially in document image analysis. Some
		  optimizations can be obtained by less generic methods: for
		  example, maps calculated by front propagation can determine
		  shorter paths, assuming that the image is non-convex. This
		  presentation discusses different distance transform
		  algorithms and underlines their advantages and weaknesses.
		  Finally we will explain our choices.},
  resume	= {Une carte de distances est une repr\'esentation sous forme
		  d'image d'une fonction distance \`a un objet. Ces cartes
		  sont utilis\'ees dans de nombreuses applications, en
		  particulier en analyse d'images de documents qui nous
		  serviront d'illustration. Certaines m\'ethodes de calcul de
		  cartes moins g\'en\'eriques que d'autres peuvent s'av\'erer
		  plus rapides : par exemple, des cartes calcul\'ees par
		  propagation de fronts permettent de d\'eterminer des plus
		  courts chemins mais ne fonctionnent que lorsque le support
		  est connu pour \^etre non-convexe. Cette pr\'esentation
		  fait un tour d'horizon des diff\'erents algorithmes de
		  calculs de cartes de distances, met en \'evidence leurs
		  atouts et faiblesses et explique les choix retenus.}
}

@TechReport{	  folio.09.seminar,
  author	= {Etienne Folio},
  title		= {Histograms},
  titre		= {Histogrammes},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2009,
  urllrde	= {200906-Seminar-Folio},
  abstract	= {A histogram is a representation of the distribution of
		  data in an image, e.g., gray-levels or colors. This common
		  tool is used for many applications and especially for
		  classification purposes. This key-feature has to be
		  generically implemented in the \textsc{Milena} library. In
		  this seminar, we propose to store histogram data using an
		  image container. To that aim, adapting the definition of
		  value types is required. More specifically, we propose to
		  augment the traits associated with value types, add some
		  new useful value types, and design new abstractions over
		  them. Last, we present how to deal with circular data, such
		  as the "hue" values (encoded by angles in the HSL color
		  space); in this case, the "histogram image" would become
		  also circular.},
  resume	= {Un histogramme est une repr\'esentation de la distribution
		  de donn\'ees dans une image, par exemple des niveaux de
		  gris ou des couleurs. Cette caract\'eristique essentielle
		  doit \^etre impl\'ement\'ee g\'en\'eriquement dans la
		  biblioth\`eque \textsc{Milena}. Durant ce s\'eminaire nous
		  proposons d'enregistrer les donn\'ees d'histogrammes dans
		  des conteneurs d'images. Pour cela, nous avons besoin
		  d'adapter la d\'efinition des types de valeurs. Plus
		  sp\'ecifiquement, nous proposons d'augmenter les traits
		  associ\'es aux types de valeurs, d'ajouter de nouveaux
		  types de valeurs utiles et de construire de nouvelles
		  abstractions au dessus. Enfin nous pr\'esenterons comment
		  traiter des donn\'ees circulaires comme les valeurs de
		  teinte (encod\'ees par des angles dans l'espace de couleurs
		  HSL); dans ce cas "l'image histogramme" deviendrait aussi circulaire.}
}

@TechReport{	  fosse.04.seminar,
  oldkeys	= {fosse.dynamic-libs.04.seminar},
  author	= {Lo\"ic Fosse},
  title		= {Dynamic use of statically typed libraries, \textsc{Just In
		  Time} compilation and other solutions},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2004,
  urllrde	= {200406-Seminar-Fosse}
}

@TechReport{	  galtier.08.seminar,
  author	= {J\'er\^ome Galtier},
  title		= {Improving {Vaucanson}'s transducers composition
		  algorithm},
  titre		= {Am\'elioration de la composition des transducteurs dans
		  {Vaucanson}},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2008,
  abstract	= {Vaucanson is a library aimed at providing easy access and
		  manipulation of common automata constructions and their
		  algorithms. As such it provides schoolbook algorithms (and
		  some others on the bleeding edge) such as determinization,
		  accessible states calculation and so on. One of them is
		  composition of transducers. This algorithm isn't from an
		  obvious kind and his implementation in Vaucanson is
		  perfectible. Improving such an algorithm implementation is
		  consequently a good way to challenge Vaucanson design
		  choices.},
  resume	= {Vaucanson est une biblioth\`eque dont un des buts est de
		  permettre un acc\`es facilit\'e \`a des automates et aux
		  algorithmes qui leur sont associ\'es. Elle met donc \`a
		  notre disposition plusieurs algorithmes standard (et
		  d'autres moins conventionnels) tels que la
		  d\'eterminisation, le calcul des \'etats accessibles etc.
		  L'un de ces algorithmes est la composition de
		  transducteurs. Celui-ci n'est pas d'une nature ais\'ee \`a
		  aborder et son impl\'ementation dans Vaucanson est
		  perfectible. Am\'eliorer l'impl\'ementation d'un tel
		  algorithme est alors un bon moyen de mettre \`a l'\'epreuve
		  certains choix de conception dans Vaucanson.},
  url		= {http://publis.lrde.epita.fr/200807-Seminar-Galtier},
  urllrde	= {200807-Seminar-Galtier}
}

@TechReport{	  galtier.09.seminar,
  author	= {J\'er\^ome Galtier},
  title		= {Remedial treatment for {Vaucanson}: an enhanced automaton
		  concept},
  titre		= {Traitement curatif pour {Vaucanson}: un renforcement du
		  concept d'automate},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2009,
  abstract	= { Vaucanson allows you to manipulate finite state machines.
		  So the modeling of these objects plays a central role
		  concerning the genericity of the library. We want to be
		  able to extend the model to support new types and to
		  specialize behaviours in order to improve performances.\\
		  We will expose what can be considered as a Gordian knot in
		  the actual modeling: for example Vaucanson is unable to
		  choose the implementation of an automaton according to one
		  of its properties. The proposed solution will reinstate a
		  sane modeling and will prevent misconceptions while
		  devising algorithm specifications. Finally, we will
		  describe among other things a collection of specializations
		  of the automaton concept and a set of improvements to the
		  model that were previously too expensive to implement.},
  resume	= { Vaucanson permet de manipuler des automates finis. La
		  mod\'elisation de ces objets occupe donc une place centrale
		  dans la g\'en\'ericit\'e de la biblioth\`eque. Nous voulons
		  pouvoir \'etendre cette mod\'elisation pour supporter de
		  nouveaux types et sp\'ecialiser des comportements afin
		  d'am\'eliorer les performances.\\ Nous exposerons ce qui
		  peut \^etre consid\'er\'e comme un v\'eritable n\oe{}ud
		  gordien dans la mod\'elisation actuelle: Vaucanson est par
		  exemple incapable de choisir une impl\'ementation pour un
		  automate en fonction d'une de ses propri\'et\'es. La
		  solution apport\'ee restaure alors une mod\'elisation saine
		  et emp\^echera des erreurs de conceptions lors de la
		  recherche de sp\'ecifications des algorithmes. Finalement,
		  nous exposerons, entre autre, une s\'erie de
		  sp\'ecialisations du concept d'automate, ainsi qu'un
		  ensemble d'am\'eliorations du mod\`ele qui \'etaient
		  autrefois trop couteuses \`a mettre en place.},
  url		= {http://publis.lrde.epita.fr/200905-Seminar-Galtier},
  urllrde	= {200905-Seminar-Galtier}
}

@TechReport{	  garcia-ballester.08.seminar,
  author	= {Jean-Philippe Garcia Ballester},
  title		= {Fictious Play},
  titre		= {\'Etude du fictitious play dans le cas d'un jeu \`a
		  fonctions d'utilit\'e identiques},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2008,
  urllrde	= {200801-Seminar-Garcia},
  abstract	= {Le fictitious play, en th\'eorie des jeux, est une r\`egle
		  d'apprentissage dans laquelle chaque joueur suppose que ses
		  adversaires jouent une strat\'egie fixe (potentiellement
		  mixte, c'est-\`a-dire une distribution de probabilit\'e sur
		  un ensemble de strat\'egies). \`A chaque tour, chaque
		  joueur joue ainsi le meilleur coup contre la strat\'egie de
		  ses adversaires, d\'etermin\'ee de mani\`ere empirique \`a
		  partir de leurs coups pr\'ec\'edents. La convergence de
		  telles strat\'egies n'est pas assur\'ee, mais on sait que
		  si il y a convergence, alors les strat\'egies jou\'ees
		  correspondront statistiquement \`a un \'equilibre de Nash.
		  Il est donc tr\`es int\'eressant de conna\^itre les
		  crit\`eres de convergence. Nous nous int\'eresserons pour
		  cette pr\'esentation au cas des jeux o\`u les fonctions
		  d'utilit\'e (le gain d'un joueur en fonction des
		  strat\'egies jou\'ees) de chaque joueur sont identiques.
		  Nous \'etudierons d'abord des r\'esultats de convergence
		  dans ce cas particulier. Afin de r\'eduire la complexit\'e
		  en temps, nous verrons une variante de cet algorithme, qui
		  consiste \`a autoriser une erreur dans la meilleure
		  r\'eponse des joueurs. Nous pr\'esenterons enfin un exemple
		  d'application du fictitious play pour r\'esoudre un
		  probl\`eme a priori non li\'e \`a la th\'eorie des jeux :
		  un probl\`eme d'optimisation, c'est-\`a-dire calculer le maximum des valeurs prises par une fonction.},
  resume	= {Fictitious play, in game theory, is a learning rule in
		  which each player presumes that his opponents are playing a
		  stationary strategy ---potentially mixed, i.e. a
		  probability distribution over a set of strategies. At each
		  round, each player thus best responds to his opponents'
		  strategy, computed empirically using their previous moves.
		  Convergence of such strategies is not always assured, yet
		  we know that if there is convergence, then the strategies
		  used will correspond statistically to a Nash-equilibrium.
		  It is thus very interesting to know when fictitious play
		  converges. We will address for this presentation the case
		  where utility functions ---a player's payoff with respect
		  to the played strategies--- of each player are identical.
		  We will first study results about convergence in this
		  special case. In order to reduce the computationnal
		  complexity we will see a modified version of this algorithm
		  that allows errors in players' best replies. We will
		  finally introduce an example of use of fictitious play to
		  solve a problem not a priori connected to game theory: an
		  optimization problem, i.e. computing the maximum of the
		  values taken by a function.}
}

@TechReport{	  garrigues.08.seminar,
  author	= {Matthieu Garrigues},
  title		= {Stage de traitement d'image au {LRDE}},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2008,
  url		= {http://publications.lrde.epita.fr/200801-Seminar-Garrigues}
		  ,
  urllrde	= {200801-Seminar-Garrigues}
}

@TechReport{	  garrigues.08.seminar.fllt,
  author	= {Matthieu Garrigues},
  title		= {Fast Level Line Transform},
  titre		= {Transformation des courbes de niveau rapide},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2008,
  abstract	= {The {Fast Level Line Transform} ({FLLT}) constructs a
		  contrast-invariant representation of an image. This
		  algorithm builds a tree which follows the inclusion of the
		  shapes contained in an image. For an image filter, having
		  the contrast-invariant property is interesting. For
		  instance, in the field of document image analysis, this
		  representation is precious to extract characters whatever
		  their mean gray-levels are brighter or darker than their
		  surroundings. This document presents how this algorithm is
		  introduced in our image processing library and shows the
		  results of some connected that can be derived from this
		  representation.},
  resume	= {La transformation rapide des courbes de niveaux ({FLLT})
		  construit une repr\'esentation d'une image ind\'ependante
		  du contraste. Cet algorithme construit un arbre suivant les
		  inclusions des formes. Pour un filtre, \^etre invariant
		  suivant le contraste est un plus. Par exemple, en analyse
		  de document, cette repr\'esentation a le pr\'ecieux
		  avantage d'extraire facilement et rapidement les
		  caract\`eres ind\'ependamment du fait qu'ils soient plus
		  clairs ou plus fonc\'es que leur voisinage. Ce document
		  pr\'esente l'introduction de l'algorithme dans notre
		  biblioth\`eque de traitement d'images et montre les
		  r\'esultats de quelques filtres connect\'es que peut
		  engendrer cette repr\'esentation.},
  url		= {http://publis.lrde.epita.fr/200806-Seminar-garrigues},
  urllrde	= {200806-Seminar-garrigues}
}

@TechReport{	  garrigues.09.seminar,
  author	= {Matthieu Garrigues},
  title		= {Tarjan Union-Find algorithm and connected operators},
  titre		= {L'algorithme Union-Find de Tarjan et les filtres
		  connect\`es},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2009,
  abstract	= {Tarjan Union-Find algorithm (TUFA) aims to build, given an
		  image, a tree representation modeling the equivalence
		  classes following a given relation. It can be derived to
		  define filters on those tree representations. TUFA is
		  currently used in Milena, our image processing library, to
		  implement connected filters. For example, closing and
		  opening related to area, volume, or height which are useful
		  to clean an image while preserving contours. This important
		  property provides a nice advantage in comparison to the
		  classical opening and closing based on erosion and
		  dilation. Another advantage of TUFA is that it can be used
		  for algorithms which feature the domain disjointness
		  property. This document presents how new connected filters,
		  in particular self-dual, have been introduced into
		  Milena.},
  resume	= {L'algorithme Union-Find de Tarjan (TUFA) produit, \`a
		  partir d'une image, un arbre repr\`esentant des classes
		  d'\`equivalences dans une image \`etant donn\`ee une
		  relation. Cette repr\`esentation peut \^etre utilis\`ee
		  pour d\`efinir des filtres. Cette m\`ethode est
		  actuellement utilis\`ee dans Milena, notre biblioth\`eque
		  de traitement d'image, pour impl\`ementer des filtres
		  connect\`es comme par exemple l'ouverture et la fermeture
		  d'aire, de volume ou encore de hauteur. Ces filtres sont
		  utilis\`es pour filtrer une image tout en pr\`eservant les
		  contours. Cette propri\`et\`e est un avantage par rapport
		  \`a l'ouverture et la fermeture bas\`ees sur l'\`erosion et
		  la dilatation. TUFA peut \^etre utilis\`e par des
		  algorithmes conservant les domaines disjoints, ce qui est
		  un second avantage int\`eressant. Ce document pr\`esente
		  une m\`ethode pour impl\`ementer une s\`erie de nouveaux
		  filtres, notamment auto duaux.},
  url		= {http://publis.lrde.epita.fr/200901-Seminar-garrigues},
  urllrde	= {200901-Seminar-garrigues}
}

@TechReport{	  gournet.04.seminar,
  author	= {Olivier Gournet},
  title		= {Progress in {C++} source preprocessing},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2004
}

@TechReport{	  gournet.06.seminar,
  author	= {Olivier Gournet and Alexandre Borghi and Nicolas Pierron},
  title		= {Parsing with {Transformers}},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2006
}

@TechReport{	  hamelin.09.seminar,
  author	= {Alex Hamelin},
  title		= {Property based class hierarchy of {Vaucanson}'s Algebra
		  module},
  titre		= {Hi\'erarchie par propri\'et\'es du module Algebra de
		  {Vaucanson}},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2009,
  abstract	= {In Vaucanson, Finite State Machines are mathematically
		  defined by an algebraic structure module called Algebra.
		  Considering the algebraic mathematical definitions,
		  however, the current design is inaccurate: some
		  hierarchical relations are false (for example, the
		  inheritance between semirings and monoids). Moreover, we
		  are unable to add new algebraic structures easily.\newline
		  Therefore, in order to give Algebra more granularity in its
		  algebraic concept definitions, it is necessary to rework
		  its current structure by introducing a property based class
		  hierarchy similar to the one presented in SCOOP. Using the
		  mathematical operator and set properties to define
		  algebraic structures, as opposed to a usual class
		  hierarchy, we would be able to specialize algorithms more
		  precisely thanks to structure property verifications, thus
		  increasing Vaucanson's performance and expressiveness.},
  resume	= {Le module de structures alg\'ebriques de Vaucanson,
		  Algebra, sert de base \`a la d\'efinition math\'ematique
		  des automates finis. Cependant la mod\'elisation actuelle
		  est inexacte du point de vue th\'eorique: les relations
		  d'h\'eritages entre certaines classes sont fausses
		  (l'h\'eritage entre les semi-anneaux et les mono\"\i{}des
		  en est le parfait exemple). D'autre part, nous ne pouvons
		  facilement l'\'etendre avec de nouvelles structures
		  alg\'ebriques.\newline Ainsi, afin de doter Algebra d'une
		  plus grande granularit\'e dans sa d\'efinition des concepts
		  alg\'ebriques, il est n\'ecessaire de retravailler sa
		  structure globale en introduisant un syst\`eme de
		  hi\'erarchie par propri\'et\'es similaire \`a celui
		  pr\'esent\'e dans SCOOP. En se basant sur les
		  propri\'et\'es des op\'erateurs et des ensembles
		  math\'ematiques pour d\'efinir la nature des structures
		  alg\'ebriques, et non sur une hi\'erarchie de classes
		  classique, nous pourrons nous permettre une
		  sp\'ecialisation plus pr\'ecise des algorithmes gr\^ace \`a
		  la garantie de propri\'et\'es sur ces structures,
		  entra\^inant ainsi un gain de performance et d'expressivit\'e important au c\oe{}ur de Vaucanson.},
  url		= {http://publis.lrde.epita.fr/200906-Seminar-Hamelin},
  urllrde	= {200906-Seminar-Hamelin}
}

@TechReport{	  hocquet.06.seminar,
  author	= {Quentin Hocquet},
  title		= {{Scool} transformation towards {C++}},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  urllrde	= {200607-Hocquet-Moulard},
  year		= 2006
}

@TechReport{	  hocquet.08.seminar,
  author	= {Beno\^it Sigoure and Quentin Hocquet},
  title		= {{revCPP} A reversible {C++} preprocessor},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2008,
  url		= {http://publications.lrde.epita.fr/200801-Seminar-Hocquet},
  urllrde	= {200801-Seminar-Hocquet},
  abstract	= {Abstract: The Transformers project aims at creating a
		  generic framework for C++ source to source transformation.
		  Source to source transformation consists in refactoring the
		  code and producing a modified source. The resulting code
		  may be reread, reused, re-modified \ldots by programmers
		  and thus must be human-readable. Moreover it should respect
		  the original coding style. This process of preserving the
		  original layout is called high fidelity program
		  transformation. Transformers targets the C/C++ language.
		  Unlike many other languages, C++ source code is
		  preprocessed to obtain the actual source code. In our
		  program transformation context we need to un-preprocess the
		  code to give back a human-readable code to the programmer.
		  This document presents the work and research carried out to
		  implement a reversible C++ preprocessor and a
		  postprocessor, i.e. a tool to obtain the original code from
		  the preprocessed one. },
  resume	= {Le but du projet Transformers est de cr\'eer un framework
		  g\'en\'erique pour de la transformation source \`a source
		  de code C++. Une transformation "source \`a source"
		  consiste \`a retravailler le code et produire un fichier de
		  code source modifi\'e. Ce code peut \^etre relu,
		  r\'e-utilis\'e, modifi\'e ... par des programmeurs et doit
		  donc \^etre lisible. De plus, il doit respecter le coding
		  style d'origine. Ce processus de pr\'eservation de la mise
		  en page est appel\'e "High fidelity program
		  transformation". Transformers cible les langages C et C++.
		  Contrairement \`a de nombreux langages, le C++ est un
		  langage pr\'eprocess\'e pour obtenir le code source
		  effectif. Dans le contexte de la transformation de
		  programmes, il faut d\'e-pr\'eprocesser le code pour le
		  rendre lisible au programmeur. Ce document pr\'esente le
		  travail de recherche que nous avons men\'e pour
		  impl\'ementer un pr\'eprocesseur de C++ r\'eversible et un
		  postprocesseur, c'est-\`a-dire un outil permettant
		  d'obtenir le code d'origine \`a partir du code pr\'eprocess\'e.}
}

@TechReport{	  jardonnet.07.seminar,
  oldkeys	= {jardonnet.07.seminar.olena.canvas},
  author	= {Ugo Jardonnet},
  title		= {Canvas in Morphological Algorithms},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2007,
  url		= {http://publications.lrde.epita.fr/200706-Seminar-Jardonnet}
		  ,
  urllrde	= {200706-Seminar-Jardonnet},
  abstract	= {Olena is a generic image processing library developed at
		  LRDE. It provides many morphological algorithms.
		  Mathematical morphology offers several powerful tools in
		  image processing and analysis.\\ Similarities appear when
		  writing morphological algorithms. Thereby, we can classify
		  those tools and then build canvases of algorithms. This
		  report presents what is a canvas and why canvases matter.
		  We will see different manners to implement canvases with
		  their pro and con arguments. Finally, we will explain which
		  canvas implementation we have chosen for Olena and why.},
  resume	= {Olena est une biblioth\`eque g\'en\'erique de traitement
		  d'images d\'evelopp\'ee au LRDE. Elle propose un grand
		  nombre d'algorithmes morphologiques. La morphologie
		  math\'ematique, offre des outils tr\`es puissants de
		  traitement et d'analyse d'images.\\ Des similarit\'es
		  apparaissant dans l'\'ecriture des algorithmes
		  morphologiques, il est possible de les classifier et,
		  ainsi, de proposer un certain nombre de "canevas"
		  d'algorithmes. Ce rapport d\'efinie ce que sont les canevas
		  et les avantages qu'ils apportent. Apres une br\`eve
		  introduction \`a la morphologie math\'ematique, cet
		  expos\'e presentera diff\'erents canevas d'algorithmes retenus par Olena.}
}

@TechReport{	  jardonnet.08.seminar,
  author	= {Ugo Jardonnet},
  title		= {Fast Image Registration},
  titre		= {Recalage d'images rapide},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2008,
  url		= {http://publications.lrde.epita.fr/200806-Seminar-jardonnet}
		  ,
  urllrde	= {200806-Seminar-jardonnet},
  abstract	= {Image registration is a process widely used in image
		  processing. Considering two measurements $A$ and $B$ of the
		  same object (say, a radiography and an magnetic resonance
		  image (MRI)), this technique estimates a transformation of
		  $A$ so that the object in $A$ becomes aligned with the
		  object in $B$. Basically this technique is able to
		  superimpose the image $A$ over the image $B$, allowing the
		  client to see mixed information. This presentation will
		  discuss the implementation of a fast image registration
		  algorithm in Milena, the \Cxx generic image processing
		  library from the Olena platform, developed at LRDE.
		  Specific techniques used to improve this process will be
		  introduced.},
  resume	= {Le recalage d'images est une technique classique en
		  traitement d'image. Soit $A$ et $B$ deux images
		  repr\'esentant le m\^eme objet (par exemple une
		  radiographie et une image \`a r\'esonance magn\'etique
		  (IRM)), on calcule une transformation de $A$ telle que le
		  recalage de l'objet dans $A$ soit align\'e sur l'objet dans
		  $B$. Typiquement, cette technique peut permettre la lecture
		  simultan\'ee de deux mesures $A$ et $B$. Cet expos\'e
		  discutera des proc\'ed\'es de recalage rapide utilis\'es
		  dans Milena, la biblioth\`eque C++ g\'en\'erique de
		  traitement d'image de la plate-forme Olena, d\'evelopp\'ee
		  au LRDE. Certaines am\'eliorations seront pr\'esent\'ees.}
}

@TechReport{	  jardonnet.09.seminar,
  author	= {Ugo Jardonnet},
  title		= {{Image reconstruction}},
  titre		= {{Reconstruction d'image}},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2009,
  url		= {http://publications.lrde.epita.fr/200901-Seminar-jardonnet}
		  ,
  urllrde	= {200901-Seminar-jardonnet},
  abstract	= {As part of a partnership with the Gustave Roussy Institut,
		  the LRDE's image processing library Milena, offers an
		  application dedicated to image reconstruction.\\ Different
		  images of the same object but obtained from different
		  modalities have to be processed. First, these images are
		  simplified. Then objects contained by these images are
		  extracted. The final step is to mix information into a
		  unique image.\\ Thereby, the process is composed of several
		  stages: image filtering, segmentation, binarization,
		  multimodal image registration and image reconstruction. The
		  presentation will especially focus on the segmentation
		  part.},
  resume	= {Dans le cadre de son partenariat avec l'institut de
		  canc\'erologie Gustave Roussy, Milena, la biblioth\`eque de
		  traitement d'image du LRDE, propose une cha\^ine de
		  traitement d\'edi\'ee \`a la reconstruction d'image.\\
		  Diff\'erentes images d'un m\^eme objet mais obtenues par
		  diff\'erents modes d'acquisitions, sont trait\'ees.
		  Celles-ci sont d'abord simplifi\'ees. On extrait ensuite
		  les objets qu'elles contiennent. La derni\`ere \'etape
		  consiste \`a construire une image recoupant les
		  informations des diff\'erentes images.\\ Cette cha\^ine se
		  d\'ecrit ainsi en plusieurs \'etapes : filtrage de l'image,
		  segmentation, binarisation, recalage d'image multimodales
		  et reconstruction d'image. L'expos\'e se concentrera
		  essentiellement sur l'etape de segmentation.}
}

@TechReport{	  lazzara.07.seminar,
  oldkeys	= {lazzara.ma.07.seminar.boosting.vcsn},
  author	= {Guillame Lazzara and Jimmy Ma},
  title		= {Boosting {V}aucanson},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2007,
  abstract	= {The work performed last year underlined the fact that the
		  overall performance issues of Vaucanson could be widely
		  improved by an internal use of hash tables and, more
		  particularly by the Multi Index from the Boost C++ library.
		  We tried to make good use of the new functionalities
		  provided by Boost. It results in the implementation of a
		  new graph structure. We present in this report the
		  different issues implied by these modifications on the
		  graph implementation and we try to answer to the new issues
		  about the genericity of Vaucanson.},
  resume	= {Suite aux s\'eminaires de l'ann\'ee derni\`ere, il en
		  ressort que les performances globales de Vaucanson
		  pouvaient largement \^etre am\'elior\'ees par l'usage de
		  tables de hachage et plus particuli\`erement les Multi
		  Index de la biblioth\`eque Boost. Pour ce s\'eminaire, nous
		  chercherons \`a tirer parti des nouvelles fonctionnalit\'es
		  offertes par Boost. Ceci impliquera l'apparition d'une
		  nouvelle impl\'ementation de graphe. Nous pr\'esenterons au
		  cours de ce s\'eminaire les enjeux induits par ces
		  changements sur l'impl\'ementation et tenterons de
		  r\'epondre au probl\`ematiques soulev\'ees par la
		  g\'en\'ericit\'e de Vaucanson.},
  url		= {http://publications.lrde.epita.fr/200705-Seminar-Lazzara-Ma}
		  ,
  urllrde	= {200705-Seminar-Lazzara-Ma}
}

@TechReport{	  lazzara.08.seminar,
  author	= {Guillaume Lazzara},
  title		= {Boosting {Vaucanson}'s genericity},
  titre		= {Booster la g\'en\'ericit\'e de {Vaucanson}},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2008,
  url		= {http://publications.lrde.epita.fr/2007312-Seminar-Lazzara}
		  ,
  urllrde	= {2007312-Seminar-Lazzara},
  resume	= {L'architecture du projet Vaucanson a \'et\'e con\,c{}ue
		  initialement autour du design pattern Element. Ce dernier a
		  l'\'enorme avantage de distinguer \`a la fois les concepts
		  et les impl\'ementations. C'est \`a dire que pour un type
		  d'automate comme les automates bool\'eens, on peut
		  th\'eoriquement avoir plusieurs impl\'ementations qui se
		  c\^otoient dans un m\^eme programme. Malgr\'e toutes ces
		  pr\'ecautions, aujourd'hui, ajouter une nouvelle structure
		  s'av\`ere tr\`es d\'elicat et remet en cause de nombreux
		  points au sein du projet. C'est pour cette raison que
		  durant ce s\'eminaire nous tenterons de r\'epondre \`a ces
		  probl\`emes. Les probl\`emes de performances qu'a pu
		  rencontrer le projet sont \'egalement une bonne motivation
		  pour s'attaquer \`a ce sujet : il est aujourd'hui
		  indispensable de proposer des nouvelles structures plus
		  efficaces, notamment impl\'ement\'ees avec la biblioth\`eque Boost.}
}

@TechReport{	  leblanc.07.seminar,
  author	= {Antoine Leblanc},
  title		= {Efficient algorithmic methods for {N}ash equilibria
		  computation},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2007,
  url		= {http://publications.lrde.epita.fr/200706-Seminar-Leblanc},
  urllrde	= {200706-Seminar-Leblanc},
  abstract	= {One of the remaining problems with Nash equilibria is the
		  lack of efficiency of best known algorithms. In general
		  case their worst complexity is $ O(4^{n}) $. Those
		  algorithms are usually old, and aren't likely to be
		  improved. This study focuses first on main algorithms and
		  methods and explains their advantages and their weaknesses.
		  It then introduces a new algorithm developed at the LRDE
		  based on a geometrical approach: a TOP computing method in
		  $ d $ dimensions.},
  resume	= {L'un des principaux probl\`emes rencontr\'es lors de la
		  recherche d'\'equilibres de Nash est le manque
		  d'efficacit\'e des principaux algorithmes. La plupart ont
		  des complexit\'es en pire cas de l'ordre de $ O(4^{n}) $.
		  Il n'est de plus que peu probable de r\'eussir \`a
		  am\'eliorer ces algorithmes, qui sont pour la plupart
		  relativement vieux. Cette \'etude d\'etaille tout d'abord
		  les algorithmes principaux en sp\'ecifiant leurs avantages
		  et inconv\'enients, puis pr\'esente un nouvel algorithme
		  d\'evelopp\'e au LRDE bas\'e sur une approche
		  g\'eom\'etrique : le calcul du TOP en dimension $ d $.}
}

@TechReport{	  leblanc.08.seminar,
  author	= {Antoine Leblanc},
  title		= {Alternate Fictitious Play study and implementation},
  titre		= {\'Etude et impl\'ementation du Fictitious Play altern\'e},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2008,
  url		= {http://publications.lrde.epita.fr/200806-Seminar-Leblanc},
  urllrde	= {200806-Seminar-Leblanc},
  resume	= {Le calcul d'un \'equilibre de Nash dans un jeu fini est un
		  probl\`eme d\'emontr\'e PPAD-complet, ce qui signifie qu'il
		  para\^it impossible de trouver une m\'ethode de calcul
		  efficace ; la complexit\'e en pire cas des algorithmes
		  usuels est $ 2^{O(n)} $ pour un jeu de taille $ n $. La
		  recherche en ce domaine s'oriente donc vers le calcul
		  d'\'equilibres approch\'es, \`a savoir des situations
		  v\'erifiant les conditions d'un \'equilibre de Nash \`a $
		  \varepsilon $ pr\`es.
		  
		  L'algorithme du \emph{Fictitious Play} s'inscrit dans cette
		  d\'emarche de recherche. Son principe est simple : \`a
		  chaque it\'eration, chacun des joueurs ``renforce'' celle
		  de ses strat\'egies pures qui est la plus efficace face \`a
		  ses adversaires. Pour certains jeux, cet algorithme
		  converge vers un \'equilibre de Nash, fournissant ainsi un
		  algorithme d'approximation efficace. La convergence ne peut
		  toutefois \^etre prouv\'ee que pour un nombre limit\'e de
		  cas.
		  
		  Pour cette raison, il est int\'eressant d'\'etudier
		  d'autres algorithmes bas\'es sur le \emph{Fictitious Play},
		  afin de trouver d'autres cas de convergence. Nous allons
		  \'etudier ici le \emph{Fictitious Play} altern\'e, dans
		  lequel seul le joueur le plus ``\'eloign\'e'' de son gain
		  optimal renforce sa strat\'egie la plus efficace.},
  abstract	= {Nash equilibria computation in finite games is a problem
		  which is known to be PPAD-complete, which means it
		  currently seems impossible to find an efficient solution ;
		  worst case complexity of well known algorithms is in $
		  2^{0(n)} $ for any game of size $ n $. For this reason,
		  research in this domain currently focuses on $ \varepsilon
		  $-equilibria, situations which approximately satisfy the
		  conditions of a Nash equilibrium.
		  
		  The \emph{Fictitious Play} algorithm fits in this approach.
		  At each iteration of this algorithm, each of the players
		  ``strengthens'' the strategy that has the highest utility
		  in the current context. For some specific game classes this
		  algorithm converges to a Nash equilibrium, therefore
		  providing an efficient approximation method. However,
		  convergence can only be proved for a small amount of game
		  classes.
		  
		  It is therefore useful to study other algorithms (based on
		  \emph{Fictitious Play}) in order to find other convergence
		  cases. In this study, we will focus on the alternate
		  \emph{Fictitious Play} algorithm, in which only one player
		  at a time strengthens one of his strategies : the player
		  which is the ``further'' from his maximum payoff.}
}

@TechReport{	  leblanc.08.seminar.comparison,
  author	= {Antoine Leblanc},
  titre		= {Comparaison entre le \emph{Fictitious Play} et le
		  \emph{Fictitious Play Altern\'e} dans le cadre des jeux \`a
		  somme nulle},
  title		= {Efficiency comparison between \emph{Fictitious Play} and
		  \emph{Alternate Fictitious Play} algorithms on the
		  restricted set of zero-sum games},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2008,
  url		= {http://publications.lrde.epita.fr/200812-Seminar-Leblanc},
  urllrde	= {200812-Seminar-Leblanc},
  resume	= {L'algorithme du \emph{Fictitious Play} est un proc\'ed\'e
		  d'apprentissage it\'er\'e utilis\'e dans le cadre de la
		  recherche des \'equilibres de Nash. Son principe est simple
		  : \`a chaque it\'eration, chacun des joueurs ``renforce''
		  celle de ses strat\'egies pures qui est la plus efficace
		  face \`a ses adversaires. Pour certains jeux, cet
		  algorithme converge vers un \'equilibre de Nash,
		  fournissant ainsi un algorithme d'approximation efficace.
		  La convergence ne peut toutefois \^etre prouv\'ee que pour
		  un nombre limit\'e de cas. L'algorithme du \emph{Fictitious
		  Play Altern\'e} (pr\'esent\'e l'ann\'ee derni\`ere) en est
		  une variante dans lequel seul le joueur le plus
		  ``\'eloign\'e'' de son gain optimal renforce sa strat\'egie
		  la plus efficace. Cette \'etude se focalisera sur une
		  comparaison de l'efficacit\'e de ces deux algorithmes dans
		  le cadre des jeux \`a somme nulle et abordera \'egalement
		  les notions de classification des jeux n\'ecessaires \`a la
		  r\'ealisation de cet objectif.},
  abstract	= {The \emph{Fictitious Play} algorithm is an iterate
		  learning process created to compute Nash equilibria. At
		  each iteration of this algorithm, each of the players
		  ``strengthens'' the strategy that has the highest utility
		  in the current context. For some specific game classes this
		  algorithm converges to a Nash equilibrium, therefore
		  providing an efficient approximation method. However,
		  convergence can only be proved for a small amount of game
		  classes. The \emph{Alternate Fictitious Play} algorithm
		  (introduced last year) is a variant in which only one
		  player at a time strengthens one of his strategies : the
		  player which is the ``further'' from his maximum payoff.
		  This study will focus on a comparison of these two
		  approaches on the restricted set of zero-sum games. It will
		  also present the notions of game classification used for
		  this comparison.}
}

@TechReport{	  lefortier.08.seminar,
  author	= {Damien Lefortier},
  title		= {Translation of an extended {LTL} into {TBGA} in {S}pot},
  titre		= {Traduction d'une {LTL} \'etendue en {TGBA} dans Spot},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2008,
  abstract	= {Spot is centered around the automata approach to model
		  checking. The library can be used to verify that every
		  behavior of a model, a transition-based generalized B\"uchi
		  automata (TGBA), satisfies a given property, expressed
		  using an linear temporal logic (LTL) formula. Spot offers
		  two translation algorithms of LTL into TGBA, one of the two
		  main stages of the approach. We present a new translation
		  into TGBA of a LTL logic which has been extended by adding
		  operators represented by finite automaton. This translation
		  allows Spot to verify properties that were not expressible
		  before.},
  resume	= {Spot repose sur l'approche automate du \emph{model
		  checking}. La biblioth\`eque permet de v\'erifier des
		  propri\'et\'es exprim\'ees en logique temporelle \`a temps
		  lin\'eaire (LTL) sur une mod\'elisation d'un syst\`eme
		  repr\'esent\'ee par un automate de B\"uchi g\'en\'eralis\'e
		  bas\'e sur les transitions (TGBA). Spot propose
		  actuellement deux algorithmes de traduction de LTL en TGBA,
		  une des deux \'etapes principales de l'approche automate.
		  Nous pr\'esentons une nouvelle traduction en TGBA d'une
		  logique LTL qui a \'et\'e \'etendue en y ajoutant des
		  op\'erateurs repr\'esent\'es par des automates finis. Cette
		  traduction permet \`a Spot de v\'erifier des propri\'et\'es
		  qui n'\'etaient pas exprimables auparavant.},
  url		= {http://publications.lrde.epita.fr/200807-Seminar-Lefortier}
		  ,
  urllrde	= {200807-Seminar-Lefortier}
}

@TechReport{	  lefortier.09.seminar,
  author	= {Damien Lefortier},
  title		= {Translation of an extended LTL into TBGA in Spot},
  titre		= {Traduction d'une LTL \'etendue en TGBA dans Spot},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2009,
  abstract	= {Spot is a model checking library centered around the
		  automata approach, which can be used to verify properties
		  expressed using LTL (Linear Temporal Logic) formul\ae{} on
		  models represented as TGBA (Transition-based Generalized
		  B\"uchi automata). The library offers two translation
		  algorithms from LTL formul\ae{} into TGBA, one of the main
		  stages of the approach. We present an extension of one of
		  these algorithms to an extended LTL where operators are
		  represented by finite automata, allowing Spot to verify
		  properties that were not expressible before. We also
		  present how we could integrate some features of PSL
		  (Property Specification Language) in our extension.},
  resume	= {Spot est une biblioth\`eque de \mc{} qui permet de
		  v\'erifier des propri\'et\'es exprim\'ees en logique
		  temporelle \`a temps lin\'eaire (LTL) sur des mod\'eles
		  repr\'esent\'es par des automates de B\"uchi
		  g\'en\'eralis\'es bas\'es sur les transitions (TGBA). Spot
		  propose actuellement deux algorithmes de traduction de LTL
		  en TGBA, une des \'etapes principales de l'approche
		  automate. Nous pr\'esentons une nouvelle traduction en TGBA
		  d'une LTL \'etendue dont les op\'erateurs sont
		  repr\'esent\'es par des automates finis, permettant ainsi
		  \`a Spot de v\'erifier des propri\'et\'es qui n'\'etaient
		  pas exprimables auparavant. Nous pr\'esenterons aussi de
		  quelles fa\c cons nous pourrions int\'egrer certaines
		  fonctionnalit\'es de PSL (Property Specification Language) \`a notre extension.},
  url		= {http://publications.lrde.epita.fr/200907-Seminar-Lefortier}
		  ,
  urllrde	= {200907-Seminar-Lefortier}
}

@TechReport{	  legrand.08.seminar,
  author	= {Antoine Legrand},
  title		= {Generalized Linear Discriminant Sequence for Speaker
		  Verification},
  titre		= {Syst\`eme de discriminants lin\'eaires pour la
		  v\'erification du locuteur},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2008,
  abstract	= {In speaker verification appplications, GMM models have an
		  important place and have shown good perfomance. Actually,
		  linear discriminant methods using support vector machines
		  (SVM) provide better results. We will focus on a linear
		  disciminant system, the SVM-GLDS. Its uses statistics
		  directly extracted from the speech features to define the
		  recognition model without using Gaussian mixture models
		  (GMMs).\\ We\^all present and compare SVM-GLDS performance
		  to SVM-GMM on NIST speaker evaluation tasks.},
  resume	= {Dans la reconnaissance du locuteur, les mod\`eles GMM
		  occupent une place tr\`es importante dans le
		  d\'eveloppement des syst\`emes performants. Les m\'ethodes
		  de discrimination lin\'eaire \`a base de SVM donnent
		  actuellement de meilleurs r\'esultats. On s'int\'eressera
		  ici \`a un syst\`eme de discriminant lin\'eaire (le
		  SVM-GLDS). Celui-ci utilise directement, sans passer par un
		  mod\`ele GMM, des statistiques issues de l'ensemble des
		  param\`etres de la parole pour d\'efinir le mod\`ele de
		  reconnaissance. On \'evaluera les performances d'un tel
		  syst\`eme sur la base de donn\'ees NIST-SRE en le comparant
		  avec les autres syst\`emes \`a base de SVM-GMM. },
  url		= {http://publications.lrde.epita.fr/200807-Seminar-Legrand},
  urllrde	= {200807-Seminar-Legrand}
}

@TechReport{	  leroi.08.seminar,
  author	= {Guillaume Leroi},
  title		= {Synchronized Tranducers},
  titre		= {Transducteurs synchronis\'es},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2008,
  url		= {http://publications.lrde.epita.fr/200712-Seminar-Leroi},
  urllrde	= {200712-Seminar-Leroi},
  resume	= {Lors de cette pr\'esentation, un algorithme de
		  resynchronisation sera d\'ecrit ainsi que son
		  impl\'ementation dans Vaucanson. De plus, des explications
		  sont donn\'ees sur l'ajout des transducteurs a d\'elai
		  born\'e, ainsi que sur les difficult\'es qui peuvent \^etre
		  rencontr\'ees lors de l'extension de la hierarchie de
		  classes de Vaucanson.}
}

@TechReport{	  lesaint.07.seminar,
  oldkeys	= {lesaint.07.seminar.xmlproposal},
  author	= {Florian Lesaint},
  title		= {{XML} Proposal and its Application in {V}aucanson},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2007,
  abstract	= {The XML Proposal presented at CIAA 2005 (Conference on
		  Implementation and Application of Automata) and updated by
		  Florent Teronnes has some lacks. For example, labels with
		  regular expressions were not clearly defined. Our work has
		  for objective to finalize the proposal of a universal
		  description format for automata to make communication
		  between various tools easier. The second part of our work
		  is to update Vaucanson to support this new format, with a
		  reimplementation of its XML parser. It allowed us to change
		  our parser from a DOM model to a SAX one, reducing memory
		  usage and improving Vaucanson's input performances.},
  resume	= {La proposition XML pr\'esent\'ee \`a CIAA 2005 (Conference
		  on Implementation and Application of Automata) et enrichie
		  depuis par Florent Terrones montrait certaines lacunes. Par
		  exemple, la gestion des expressions rationnelles n'y
		  \'etait pas clairement d\'efinie. Ce travail avait pour but
		  de finaliser la proposition d'un format universel pour la
		  description d'automates afin de faciliter la communication
		  entre les divers outils qui leur sont consacr\'es et de
		  reviser le parser de Vaucanson.},
  url		= {http://publications.lrde.epita.fr/200744-Seminar-Lesaint},
  urllrde	= {200744-Seminar-Lesaint}
}

@TechReport{	  lesaint.08.seminar,
  author	= {Florian Lesaint},
  title		= {{FSMXML} and its application in {V}aucanson},
  titre		= {{FSMXML} et son utilisation dans {V}aucanson},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2008,
  abstract	= {Last year, we started to work on a new proposal of an XML
		  automata description format, now called FSMXML. This year
		  we are presenting a final version of our work. It takes the
		  form of an \emph{rfc}. FSMXML mainly includes a full
		  generalized regular expressions support, can describe any
		  kind of automaton and has been made easier to support. We
		  redesigned the \textsc{Vaucanson} XML parser structure to
		  get rid of a bad management of dependencies. It is updated
		  according to the \emph{rfc}.},
  resume	= {Nous avions commenc\'e l'ann\'ee derni\`ere \`a travailler
		  sur une nouvelle proposition de format XML de description
		  d'automates, devenu FSMXML. Nous pr\'esentons cette ann\'ee
		  une version aboutie de ce travail sous forme de \emph{rfc}.
		  FSMXML comprend notamment une gestion compl\`ete des
		  expressions rationnelles g\'en\'eralis\'ees, il permet de
		  d\'ecrire n'importe quel type d'automate et sa gestion est
		  facilit\'ee. Nous avons repens\'e la structure du parseur
		  XML de \textsc{Vaucanson} pour s'affranchir d'une mauvaise
		  gestion de d\'ependances et l'avons mise \`a jour
		  conform\'ement \`a la \emph{rfc}.},
  url		= {http://publis.lrde.epita.fr/200806-Seminar-Lesaint},
  urllrde	= {200806-Seminar-Lesaint}
}

@TechReport{	  lesaint.08.seminar.syncrelations,
  author	= {Florian Lesaint},
  title		= {Synchronous relations in {V}aucanson},
  titre		= {Les relations synchrones dans {V}aucanson},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2008,
  abstract	= {Synchronous rational relations is the largest subfamily of
		  rational relations so far defined that is an effective
		  Boolean algebra. It can be therefore of some interest to
		  provide manipulation tools which might help in their study.
		  With the recently added support of pair-alphabets in
		  Vaucanson, we suggest a new approach to deal with
		  synchronous rational relations represented as
		  letter-to-letter transducers and provide the necessary
		  tools to work with them.},
  resume	= {La famille des relations rationnelles synchrones est la
		  plus grande famille de relations rationnelles, d\'efinie
		  jusqu'ici, qui est une alg\`ebre de Boole effective.
		  Fournir des outils permettant de les manipuler peut donc
		  s'av\'erer int\'eressant pour leur \'etude. Le r\'ecent
		  support des alphabets de paires dans Vaucanson nous permet
		  une nouvelle approche pour travailler avec les relations
		  rationnelles synchrones, repr\'esent\'ees par des
		  transducteurs lettre \`a lettre, pour lesquels nous
		  fournissons les outils n\'ecessaires \`a leur manipulation.},
  url		= {http://publications.lrde.epita.fr/200901-Seminar-Lesaint},
  urllrde	= {200901-Seminar-Lesaint}
}

@TechReport{	  ma.08.seminar,
  author	= {Jimmy Ma},
  title		= {Boosting {V}aucanson's Iterator},
  titre		= {Booster les it\'erateurs de {V}aucanson},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2008,
  abstract	= {Vaucanson is a generic finite state machine manipulation
		  platform. We have based our genericity on the ability to
		  not only support various types of automata, but also to use
		  different data structures to represent them. In its current
		  state, we have various techniques to iterate over sets of
		  transitions, however, none of them is really independent of
		  the data structures. To overcome this problem, we have
		  integrated the design pattern Iterator. Our goal is to
		  assess the improvements given by this method in terms of
		  performance and code writing.},
  resume	= {Vaucanson est une biblioth\`eque g\'en\'erique de
		  manipulation d'automates. Le c\oe{}ur de sa
		  g\'en\'ericit\'e r\'eside dans le support de types
		  d'automates vari\'es mais aussi sa capacit\'e \`a s'appuyer
		  sur diff\'erentes structures de donn\'ees. Actuellement,
		  nous avons diff\'erentes mani\`eres de manipuler des
		  transitions. Cependant, aucune d'entre elles n'est
		  r\'eellement ind\'ependante de la structure de donn\'ees
		  utilis\'ee. Afin de pallier cela, nous allons nous tourner
		  vers le design pattern Iterator. Nous \'evaluerons l'impact
		  de ce design pattern sur les performances et sur
		  l'utilisation de la biblioth\`eque en termes d'\'ecriture d'algorithmes.},
  url		= {http://publis.lrde.epita.fr/200806-Seminar-Ma},
  urllrde	= {200806-Seminar-Ma}
}

@TechReport{	  ma.09.seminar,
  author	= {Jimmy Ma},
  title		= {{A}utomata in {N}atural {L}anguage {P}rocessing},
  titre		= {Les automates en traitement automatique des langues
		  naturelles},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2009,
  abstract	= {Vaucanson has been designed to satisfy the needs of the
		  automaticians. There are, however, other fields where
		  automata are widely used. This prospective report will
		  focus on linguistics, and more precisely the automata needs
		  of the natural language processing community since the
		  applications differ from our usual ones. This way, we will
		  be able to assess our range of applicability and usability.
		  First, we will explore the different domains and detail the
		  various needs. Then, we will be able to evaluate whether or
		  not Vaucanson is usable for those domains and, finally,
		  discuss some potential work leads.},
  resume	= {Jusqu'\`a pr\'esent, Vaucanson s'adressait essentiellement
		  \`a la communaut\'e des automaticiens. Cependant, il existe
		  d'autres domaines o\`u les automates sont utilis\'es. Dans
		  ce rapport prospectif, nous nous orienterons vers la
		  communaut\'e des linguistes et, plus pr\'ecis\'ement, vers
		  le domaine du traitement automatique des langues naturelles
		  car les besoins de ce domaine diff\`erent de nos
		  utilisations habituelles. Nous observerons diverses
		  sp\'ecialit\'es et dresserons un bilan des besoins en
		  automates. Ainsi, nous pourrons \'evaluer l'ad\'equation de
		  Vaucanson pour ce domaine et en d\'egager des pistes
		  d'\'evolutions \'eventuelles pour le projet.},
  url		= {http://publis.lrde.epita.fr/200901-Seminar-Ma},
  urllrde	= {200901-Seminar-Ma}
}

@TechReport{	  melin.07.seminar,
  oldkeys	= {melin.ramakichenin.07.seminar},
  author	= {Charles Melin and Julien Ramakichenin},
  title		= {{LRDE}'s Speaker Verification Framework},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2007,
  url		= {http://publications.lrde.epita.fr/200706-Seminar-Ramakichenin-Melin-Report}
		  ,
  urllrde	= {200706-Seminar-Ramakichenin-Melin-Report}
}

@TechReport{	  moulard.06.seminar,
  author	= {Thomas Moulard},
  title		= {Conception of a static oriented language: an overview of
		  {Scool}},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  urllrde	= {200607-Hocquet-Moulard},
  year		= 2006
}

@TechReport{	  moulard.07.seminar,
  oldkeys	= {moulard.07.seminar.scoop.container},
  author	= {Thomas Moulard},
  title		= {{C++} container library with the {SCOOP} paradigm},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2007,
  url		= {http://publications.lrde.epita.fr/200706-Seminar-Moulard},
  urllrde	= {200706-Seminar-Moulard}
}

@TechReport{	  moulard.08.seminar,
  author	= {Thomas Moulard},
  title		= {An overview of {Scoop}, a static object-oriented
		  paradigm},
  titre		= {Une introduction \`a {Scoop}, un paradigme \Cxx orient\'e
		  objet},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2007,
  url		= {http://publications.lrde.epita.fr/200712-Seminar-Moulard},
  urllrde	= {200712-Seminar-Moulard},
  abstract	= {\Cxx has achieved to support classic object-oriented and
		  generic programming, but some modelisation problems remain
		  recurrent and difficult to solve. Scoop is a static
		  Object-Oriented paradigm. The paradigm provides virtual
		  methods, argument covariance, virtual types and
		  multi-methods statically typed without extending the
		  language. Scoop uses its own \Cxx library in order to make
		  easier to design a library with this paradigm.},
  resume	= {\Cxx a r\'eussi \`a supporter \`a la fois la programmation
		  orient\'e objet classique et la programmation
		  g\'en\'erique, cependant certains probl\`emes r\'ecurrents
		  restent toujours difficiles \`a r\'esoudre. Scoop est un
		  paradigme orient\'e objet. Il fournit des m\'ethodes
		  virtuelles, les arguments covariants, les types virtuels et
		  les multi-m\'ethodes typ\'ees statiquement sans avoir
		  besoin d'\'etendre le langage. Scoop utilise sa propre
		  biblioth\`eque \Cxx pour facilier la conception d'une
		  biblioth\`eque utilisant ce paradigme.}
}

@TechReport{	  neri.07.seminar,
  oldkeys	= {neri.07.seminar.learning},
  author	= {Neri Nicolas},
  title		= {Learning models for model-checking},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2007,
  url		= {http://publications.lrde.epita.fr/20070509-Seminar-NicolasNeri-Report}
		  ,
  urllrde	= {20070509-Seminar-NicolasNeri-Report}
}

@TechReport{	  neri.08.seminar,
  author	= {Nicolas Neri},
  title		= {Transfinite Chomp},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2007,
  url		= {http://publications.lrde.epita.fr/200801-Seminar-Neri},
  urllrde	= {200801-Seminar-Neri},
  abstract	= {FILL ME},
  resume	= {Dans ce rapport technique, nous nous attardons sur le jeu
		  de la tablette de chocolat. On dispose d'une tablette de
		  chocolat dont le carr\'e inf\'erieur gauche est
		  empoisonn\'e. Les joueurs jouent \`a tour de r\^ole. Un
		  coup consiste \`a choisir un carr\'e de chocolat et \`a le
		  manger ainsi que tous les carr\'es qui sont \`a sa droite
		  et au dessus de lui. Le joueur qui mange le carr\'e
		  empoisonn\'e perd la partie. Dans cet expos\'e, nous nous
		  int\'eresserons particuli\`erement au cas o\`u les
		  dimensions du jeu sont de classe cardinale infinie. On
		  pr\'esentera \'egalement, pour une meilleure
		  compr\'ehension, les nombres ordinaux et leur ordre associ\'e.}
}

@TechReport{	  o-connor.04.seminar,
  oldkeys	= {o-connor.transducer.04.seminar},
  author	= {Sarah O'Connor},
  title		= {Implementation of transducers in {V}aucanson},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2004,
  urllrde	= {20040623-Seminar-SarahOConnor-transducers-Slides}
}

@TechReport{	  odou.05.seminar,
  oldkeys	= {odou.taxonomy.05.seminar},
  author	= {Simon Odou},
  title		= {Images taxonomy and modeling},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2005,
  urllrde	= {200506-Seminar-Odou}
}

@TechReport{	  ordy.08.seminar,
  author	= {Vincent Ordy},
  title		= {Implementing a {C++} extension with {Transformers}:
		  \texttt{class namespace}},
  titre		= {Impl\'ementation d'une extension du {C++} dans
		  {Transformers}: \texttt{class namespace}},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2008,
  astract	= {C++ classes are closed, such that once a class definition
		  is ended, nothing can be added to it. But most of the time,
		  programmers are used to distinguish method definition from
		  method implementation. As a consequence, using
		  fully-qualified name of method names and return types are
		  needed, which is repetitive and tedious, especially with
		  template and nested classes. We propose extending C++
		  grammar with a namespace-like syntax in order to define
		  easily member functions and static data members already
		  declared in the class definition. This work will be based
		  on Tranformers' C++ grammar and transformation rules in
		  Stratego Language.},
  resume	= {Les classes en C++ sont ferm\'ees, c'est-\`a-dire qu'on ne
		  peut rien leur ajouter une fois leur d\'efinition
		  termin\'ee. Or, la plupart du temps, les programmeurs
		  s\'eparent la d\'efinition de l'impl\'ementation, ce qui
		  oblige \`a utiliser une syntaxe r\'ep\'etitive, en
		  particulier dans le cas de patrons de classes ou de classes
		  imbriqu\'ees. On se propose donc de faire une extension de
		  la grammaire du C++ permettant via une syntaxe proche de
		  celle des namespaces de d\'efinir plus ais\'ement des
		  m\'ethodes ou attributs statiques d\'ej\`a d\'eclar\'es
		  dans la d\'efinition de la classe. Dans ce but, nous
		  utiliserons la grammaire du C++ impl\'ement\'ee dans
		  Transformers, et des transformations \'ecrites en Stratego.},
  url		= {http://publications.lrde.epita.fr/200807-Seminar-Ordy},
  urllrde	= {200807-Seminar-Ordy}
}

@TechReport{	  ordy.09.seminar,
  author	= {Vincent Ordy},
  title		= {Adding {Contracts} to {C++} with {Transformers}},
  titre		= {Ajout de la programmation par contrats au {C++} avec
		  {Transformers}},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2009,
  abstract	= {Contract programming is a paradigm that allows developers
		  to ensure that some conditions are satisfied when a
		  function is called (\emph{preconditions}), or when it
		  returns (\emph{postconditions}). It makes debugging easier
		  and it is a good way to document what a function accepts as
		  argument or ensures on the result. In oriented-object
		  languages, it is also possible to check a set of conditions
		  on every call and return of any member function of a class
		  (\emph{class invariants}), and conditions are inherited
		  from parent classes.
		  
		  The Transformers project aims at providing source to source
		  transformations on C and C++ sources. A C extension to
		  support contracts in C has already been written, when C++
		  parsing wasn't working yet.
		  
		  We will show how to write a C++ grammar extension to bring
		  contracts in the C++ with the Transformers project, and
		  then how to transform this extended C++ into standard C++.},
  resume	= {La programmation par contrats est un paradigme permettant
		  de r\'eduire le temps de d\'ebogage des programmes, en
		  sp\'ecifiant des conditions qui doivent \^etre
		  v\'erifi\'ees \`a l'entr\'ee d'une fonction (\emph
		  {pr\'econditions}) ou \`a sa sortie (\emph
		  {postconditions}). Dans les languages orient\'es objet, il
		  est \'egalement possible de v\'erifier un ensemble de
		  contraintes \`a chaque appel ou sortie d'une fonction
		  membre (\emph{invariants de classe}), et les conditions
		  sont h\'erit\'ees \`a partir des classes parentes.
		  
		  Le projet Transformers a pour but de faire de la
		  transformation source \`a source sur des sources C et C++.
		  Une extension pour introduire les contrats dans le C a
		  d\'ej\`a \'et\'e \'ecrite alors que l'analyse syntaxique du
		  C++ n'\'etait pas encore fonctionnelle.
		  
		  Nous allons montrer comment \'ecrire une extension de la
		  grammaire du C++ dans le projet Transformers pour
		  introduire le principe de contrat dans le C++, puis
		  transformer le code \'etendu en C++ standard.},
  url		= {http://publications.lrde.epita.fr/200905-Seminar-Ordy},
  urllrde	= {200905-Seminar-Ordy},
  number	= 0901
}

@TechReport{	  pierron.07.seminar,
  oldkeys	= {pierron.07.seminar.formal.def},
  author	= {Nicolas Pierron},
  title		= {Formal Definition of the Disambiguation with Attribute
		  Grammars},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2007,
  url		= {http://publications.lrde.epita.fr/200706-Seminar-Pierron},
  urllrde	= {200706-Seminar-Pierron},
  astract	= {The current problem of the disambiguation in Transformers
		  with attribute grammars is that no-one has a proof that
		  allows certification of this approach. The current use of
		  attribute grammars for the disambiguation of C and a part
		  of C++ lets us think that this method is correct. In order
		  to remove any doubt, a definition and a formalization of
		  our approach are necessary. This work is split in two
		  parts. The first one relates to the proof of the validity
		  of the approach used in Transformers. The second one is
		  devoted to the correction and the re-development of the
		  existing tools according to the model defined.},
  resume	= {Le probl\`eme actuel de la d\'esambigu\"isation dans
		  Transformers avec des grammaires attribu\'ees est que l'on
		  ne poss\`ede pas de preuve permettant de certifier cette
		  approche. L'usage actuel des grammaires attribu\'ees pour
		  la d\'esambigu\"isation du C et d'une partie du C++ laisse
		  \`a penser que cette m\'ethode est correcte. Afin de
		  supprimer tout doute, une d\'efinition et une formalisation
		  de notre approche est n\'ecessaire. Ce travail comporte est
		  divis\'e en deux parties. La premi\`ere porte sur la preuve
		  de la validit\'e de l'approche utilis\'ee dans
		  Transformers. La seconde est consacr\'ee \`a la correction
		  et au re-d\'eveloppement des outils existants suivant le mod\`ele d\'efini.}
}

@TechReport{	  pierron.08.seminar,
  author	= {Akim Demaille and Renaud Durlin and Nicolas Pierron and
		  Beno\^it Sigoure},
  title		= {Automatic Attribute Propagation for Modular Attribute
		  Grammars},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2008,
  url		= {http://publications.lrde.epita.fr/200801-Seminar-Pierron},
  urllrde	= {200801-Seminar-Pierron},
  astract	= {Attribute grammars are well suited to describe (parts of)
		  the semantics of programming languages: hooked on the
		  syntactic production rules, they allow to express local
		  relations that are later bound globally by a generic
		  evaluator. However they fall short when working on large
		  and complex languages. First attributes that are virtually
		  needed everywhere need to be propagated by every single
		  production rule. Second, this constraint hinders
		  modularity, since adding a syntactic extension might
		  require the propagation of new attributes in the rest of
		  the language. This paper shows how to solve these problems
		  by introducing a technique to automatically propagate
		  attributes by completing the set of semantic rules. We
		  formally define the propagation constraints such that it is
		  optimized to avoid unnecessary addition of semantic rules.
		  Attribute grammars are thus made more maintainable, modular
		  and easier to use.},
  resume	= {Les grammaires attribu\'ees sont plus adapt\'ees pour
		  d\'ecrire (des parties de) la s\'emantique d'un langage de
		  programmation : accroch\'ees sur les r\`egles de production
		  syntaxique, elles permettent d'exprimer des relations
		  locales qui sont par la suite li\'ees entre elles
		  globalement par un \'evaluateur g\'en\'erique. Cependant
		  elles ne passent pas \`a l'\'echelle quand on travaille
		  avec des langages volumineux et complexes. Premi\`erement
		  les attributs qui sont requis quasiment partout ont besoin
		  d'\^etre v\'ehicul\'es par chaque r\`egle de production.
		  Deuxi\`emement, ces contraintes cassent la modularit\'e car
		  le fait d'\'etendre une grammaire n\'ecessite la
		  propagation des nouveaux attributs \`a travers le reste du
		  langage. Ce papier montre comment r\'esoudre ces
		  probl\`emes en introduisant un syst\`eme de propagation
		  automatique des attributs qui compl\`ete l'ensemble des
		  r\`egles s\'emantiques. Nous avons d\'efini formellement
		  les contraintes de propagations de mani\`ere optimis\'ee
		  afin d'\'eviter l'ajout de r\`egles s\'emantiques inutiles.
		  Ainsi les grammaires attribu\'ees sont devenus plus
		  maintenables, modulaires et facile \`a utiliser.}
}

@TechReport{	  pouchet.04.seminar.gui,
  oldkeys	= {pouchet.vgi.04.seminar},
  author	= {Louis-Noel Pouch\"et},
  title		= {D\'eveloppement d'une interface graphique pour
		  {V}aucanson},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2004,
  note		= {Core curriculum internship of EPITA},
  urllrde	= {FIXME}
}

@TechReport{	  pouchet.04.seminar.interpreter,
  oldkeys	= {pouchet.vcsn-interpretor.04.seminar},
  author	= {Louis-No\"el Pouchet},
  title		= {An interpreter for {V}aucanson},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2004,
  urllrde	= {20040623-Seminar-Pouchet-Vaucanson-Interpreter-Report}
}

@TechReport{	  querol.07.seminar,
  author	= {Geoffroy Querol},
  title		= {Speaker recognition evaluation: selective approaches and
		  fusion},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2007,
  url		= {http://publications.lrde.epita.fr/200705-Seminar-Querol},
  urllrde	= {200705-Seminar-Querol}
}

@TechReport{	  querol.08.seminar,
  author	= {Geoffroy Querol},
  title		= {{SVM-MLLR} for multi-speaker verification systems score
		  fusion},
  titre		= {{SVM-MLLR} et fusion pour la v\'erification du locuteur},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2008,
  url		= {http://publications.lrde.epita.fr/200756-Seminar-Querol},
  urllrde	= {200756-Seminar-Querol},
  resume	= {Afin d'am\'eliorer la performance globale des syst\`emes
		  de v\'erification du locuteur, il faut diversifier les
		  approches. Le but de ce travail est d'\'etudier les
		  performances d'un syst\`eme SVM-MLLR. Cette m\'ethode se
		  base sur la construction, \`a partir du mod\`ele du monde,
		  d'une transformation lin\'eaire des vecteurs moyennes (mean
		  supervectors) maximisant la vraisemblance du mod\`ele
		  transform\'e par rapport aux donn\'ees locuteur. On
		  \'evaluera deux approches diff\'erentes : Dans la
		  premi\`ere, on utilisera directement le logarithme du
		  rapport de vraisemblance (GMM-MLLR). Dans une deuxi\`eme
		  exp\'erimentation, on utilisera les SVMs pour \'evaluer les
		  scores de d\'ecision. La derni\`ere \'etape consiste \`a
		  valuer l'apport d'une m\'ethode de compensation du canal
		  (NAP: Nuisance Attribute Projection) sur les performances
		  de ce syst\`eme). Une fusion des scores avec d'autres
		  syst\`emes GMM sera \'etudi\'ee. Une fusion au niveau des
		  noyaux sera quand \`a elle pr\'esent\'ee par Charles-Alban.
		  Tous les tests vont \^etre men\'es sur les deux bases de
		  donn\'ees NIST-SRE 2005 et 2006 all trials.}
}

@TechReport{	  queze.07.seminar,
  oldkeys	= {queze.07.seminar.tools.ag.manipulation},
  author	= {Florian Qu\`eze},
  title		= {Tools for Attribute Grammars manipulation in
		  Transformers},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2007,
  url		= {http://publications.lrde.epita.fr/200706-Seminar-Queze},
  urllrde	= {200706-Seminar-Queze}
}

@TechReport{	  queze.08.seminar,
  author	= {Florian Qu\`eze},
  title		= {C++ Program Slicing with {Transformers}},
  titre		= {D\'ecoupage de programme C++ avec {Transformers}},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2008,
  url		= {http://publis.lrde.epita.fr/200806-Seminar-Queze},
  urllrde	= {200806-Seminar-Queze},
  abstract	= {Transformers is a C++ manipulation framework built on
		  Stratego/XT. Program Slicing is an important field of
		  program transformation. We will explain what Program
		  Slicing is, give a quick overview of various aspects of
		  Program Slicing and show how Transformers can be turned
		  into a C++ Program Slicing tool.},
  resume	= {Transformers est un emsemble d'outils bas\'es sur
		  Stratego/XT permettant la manipulation de programmes C++.
		  Le d\'ecoupage de programmes est un domaine important de la
		  transformation de programmes. Nous allons expliquer ce
		  qu'est le d\'ecoupage de programmes, donner un aper{\c c}u
		  rapide de ses diff\'erents aspects et montrer comment
		  Transformers pourrait \^etre utilis\'e comme un outil
		  permettant le d\'ecoupage de programmes.}
}

@TechReport{	  queze.08.seminar.transend,
  author	= {Florian Qu\`eze},
  title		= {{Transformers}: toward the end of the pipeline},
  titre		= {{Transformers} : vers la fin du tunnel},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2008,
  url		= {http://publications.lrde.epita.fr/URLLRDE},
  abstract	= {The goal of the {Transformers} project is to create a C
		  and C++ manipulation framework. Once achieved, it will
		  simplify the creation of transformation or analysis tools
		  for the C++ language. It will also allow experiments with
		  language extensions.
		  
		  {Transformers}' students have been working for years on the
		  project, handing over, each year, the results of their work
		  to the next generation. Lots of tools have been created,
		  each of them solving (or attempting to) a problem that we
		  face when trying to manipulate C++ code. Tens of reports
		  have been written. Some are outdated, but some of them are
		  still a valuable resource.
		  
		  It's now time to take a step back and to have a look at
		  what has been accomplished and what does not work yet in
		  order to decide if and how we want to go forward on this
		  project.},
  resume	= {Le but du projet {Transformers} est de cr\'eer un ensemble
		  d'outils de manipulation de C et C++. Une fois achev\'e, il
		  simplifiera la cr\'eation d'outils de transformation ou
		  d'analyse pour le C++. Il permettra aussi d'exp\'erimenter
		  des extensions du langage.
		  
		  Les \'etudiants du groupe {Transformers} ont travaill\'e
		  pendant des ann\'ees, passant chaque ann\'ee le r\'esultat
		  de leur travail \`a la g\'en\'eration suivante. De nombreux
		  outils ont \'et\'e cr\'e\'es, chacun d'eux r\'esolvant (ou
		  tentant de r\'esoudre) un probl\`eme auquel nous sommes
		  confront\'es en essayant de manipuler du C++. Des dizaines
		  de rapports ont \'et\'e \'ecrits : certains sont
		  obsol\`etes, mais d'autres sont des ressources pr\'ecieuses.
		  
		  Il est maintenant temps de prendre du recul, de regarder le
		  travail accompli et ce qui ne fonctionne pas encore, afin
		  de d\'ecider si et comment nous voulons poursuivre ce projet.},
  urllrde	= {URLLRDE}
}

@TechReport{	  raud.08.seminar,
  author	= {C\'edric Raud},
  title		= {{C}entaur: A generic framework simplifying {C++}
		  transformation},
  titre		= {{C}entaur : Une infrastructure g\'en\'erique simplifiant
		  les transformations de {C++}},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2008,
  url		= {http://lrde.org/cgi-bin/twiki/view/Publications/200807-Seminar-Raud}
		  ,
  urllrde	= {200807-Seminar-Raud},
  abstract	= {The C++ standard grammar was not thought to be easily
		  parsable so its use in the context of program handling can
		  be compared to the complexity of the AST generated by it.
		  The aim of Centaur inside Tranformers is to propose a
		  generic framework allowing to manipulate and digest this
		  AST : program transformations are simplified thanks to an
		  easier access to the parse tree data and its annotations.
		  Thanks to this library, repetitive and error-prone tasks
		  like enumerating a container's elements or the parent
		  classes lookup of a class will be factorized by a function
		  set corresponding to an extensible and modular model.},
  resume	= {La grammaire du standard du C++ n'ayant pas \'et\'e
		  con\c{c}ue pour etre ais\'ement analysable, son utilisation
		  dans le cadre de la manipulation de programme est
		  comparable \`a la complexit\'e de l'AST g\'en\'er\'e par
		  celle-ci. Le r\^ole de Centaur au sein de Transformers est
		  ainsi de fournir une infrastructure g\'en\'erique
		  permettant de manipuler et de synth\'etiser cet AST : les
		  transformations de programmes sont simplifi\'ees gr\`ace a
		  un acc\`es plus ais\'e aux informations contenues dans
		  l'arbre syntaxique et ses annotations. Gr\^ace \`a cette
		  bibliotheque, les t\^aches r\'ep\'etitives et souvent
		  g\'en\'eratrices d'erreurs, comme l'\'enum\'eration des
		  \'el\'ements d'un conteneur ou la recherche des classes
		  parentes d'une classe, seront factoris\'ees par un ensemble
		  de fonctions correspondant \`a un mod\`ele modulaire et extensible.}
}

@TechReport{	  roussel.04.seminar,
  author	= {Julien Roussel},
  title		= {\textsc{Transformers}: {C++} type-checking},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2004
}

@TechReport{	  roussel.04.seminar2,
  author	= {Julien Roussel},
  title		= {{C++} type-checking: A study of existing solutions},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2004
}

@TechReport{	  sadegh.08.seminar,
  author	= {Guillaume Sadegh},
  title		= {A Promela front-end for Spot},
  titre		= {Front-end Promela dans Spot},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2008,
  abstract	= {Spot is a C++ library for model checking. For
		  verification, Spot uses an input-format which describes a
		  Transition-based Generalized B\"uchi Automata (TGBA).
		  However, this format doesn't seem accessible for users with
		  its poor abstraction and the need for often representing
		  automaton with millions of states. Promela (Process
		  Meta-Language) is a verification modeling language used by
		  the Spin model checker. It lets users to describe a
		  parallel system for verification in a high level
		  programming language. We will present different ways to add
		  a Promela front-end in Spot, which will allow to explore
		  the state-graph on-the-fly, in order to avoid storing all
		  the states.},
  resume	= {Spot est une biblioth\`eque de model checking. Pour
		  v\'erifier des mod\`eles, Spot utilise un format d'entr\'ee
		  repr\'esentant des automates de B\"uchi g\'en\'eralis\'es
		  bas\'es sur les transitions (TGBA). Ce format est peu
		  pratique pour des utilisateurs, par son manque
		  d'abstraction et par la taille des automates \`a
		  repr\'esenter, souvent compos\'es de millions d'\'etats.
		  Promela (Process Meta-Language) est un langage de
		  sp\'ecification de syst\`emes asynchrones, utilis\'e par le
		  model checker Spin. Il permet de repr\'esenter des
		  syst\`emes concurrents dans un langage imp\'eratif de haut
		  niveau. Nous allons pr\'esenter plusieurs approches pour
		  l'ajout d'un front-end Promela dans Spot, qui devront
		  permettre une exploration \`a la vol\'ee du graphe
		  d'\'etats, afin d'\'eviter de conserver en m\'emoire tous les \'etats.},
  url		= {http://publications.lrde.epita.fr/200807-Seminar-Sadegh},
  urllrde	= {200807-Seminar-Sadegh}
}

@TechReport{	  sadegh.09.seminar,
  author	= {Guillaume Sadegh},
  title		= {Complementing {B\"uchi} Automata},
  titre		= {La compl\'ementation d'automates de {B\"uchi}},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2009,
  abstract	= {Model checking is a field of formal verification which
		  aims to automatically test the behavior of a system with
		  the help of logic formulae. Spot is a model checking
		  library which relies on one technique: the
		  automata-theoretic approach. In this approach, both system
		  and formula are expressed with {B\"uchi} automata, which
		  are automata on infinite words. Spot provides several
		  algorithms to deal with these automata, with model checking
		  as objective. However, an algorithm is missing: the
		  complementation of {B\"uchi} automata. Because of its high
		  complexity this algorithm is rarely used in practical, but
		  it does not lack theoretical interests. We will present an
		  implementation of this algorithm in Spot. },
  resume	= {Le model checking est un domaine de la v\'erification
		  formelle, qui permet de v\'erifier le comportement d'un
		  syst\`eme \`a travers des formules logiques. Spot est une
		  biblioth\`eque de model checking qui repose sur une des
		  techniques du domaine : l'approche automate. Dans cette
		  approche du model checking, le syst\`eme et les formules
		  sont repr\'esent\'es sous forme d'automates acceptant des
		  mots de longueur infinie, et plus particuli\`erement des
		  automates de {B\"uchi}. Spot propose de nombreux
		  algorithmes aux utilisateurs de la biblioth\`eque pour
		  manipuler ce type d'automates, en vue d'applications au
		  model checking. Pourtant, un algorithme est manquant :
		  celui de la compl\'ementation d'automates de {B\"uchi} (qui
		  produit un automate reconnaissant la n\'egation du langage
		  initialement reconnu). Cet algorithme est peu utilis\'e
		  dans la pratique \`a cause de sa forte complexit\'e, mais
		  il ne manque pas d'int\'er\^et du point de vue th\'eorique.
		  Nous pr\'esenterons une impl\'ementation d'un tel algorithme dans Spot.},
  url		= {http://publications.lrde.epita.fr/200906-Seminar-Sadegh},
  urllrde	= {200906-Seminar-Sadegh}
}

@TechReport{	  seine.08.seminar,
  author	= {Warren Seine},
  titre		= {D\'esambigu\"isation des patrons de type C++ avec les
		  Grammaires Attribu\'ees de Transformers},
  title		= {C++ template disambiguation with {Transformers} Attribute
		  Grammars},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2008,
  url		= {http://publications.lrde.epita.fr/200807-Seminar-Seine},
  urllrde	= {200807-Seminar-Seine},
  abstract	= {C++ is a context-sensitive language that can be parsed
		  using a context-free but ambiguous grammar. Disambiguation
		  is then required to select the unique semantically-valid
		  parse tree. Transformers, a framework for C++ program
		  transformation, uses attribute grammars to achieve that
		  stage.
		  
		  One of the hardest ambiguity in the language is related to
		  metaprogramming. In so far as code is generated when
		  templates are instanciated, types are not fully known at
		  the declaration site. Therefore, type-checking is needed to
		  perfectly handle templates, and it poses a real challenge.
		  
		  This report focuses on template disambiguation, detailing
		  the problems and how to resolve it, in order to provide a
		  better platform for source manipulation.},
  resume	= {Malgr\'e sa sensibilit\'e au contexte, le C++ est
		  analysable avec une grammaire hors-contexte mais ambig\"ue.
		  La d\'esambigu\"isation est ensuite n\'ec\'essaire pour
		  s\'electionner le seul arbre syntaxique s\'emantiquement
		  valide. Transformers est une collection d'outils pour la
		  transformation de programmes C++ qui utilise les grammaires
		  attribu\'ees pour r\'ealiser cette \'etape.
		  
		  Une des plus difficiles ambiguit\'es dans le langage
		  concerne la m\'eta-programmation. Puisque du code est
		  g\'en\'er\'e \`a l'instanciation, tous les types ne sont
		  pas n\'ec\'essairement connus \`a la d\'eclaration. La
		  v\'erification des types est donc obligatoire pour traiter
		  totalement le cas des patrons, ce qui pose un v\'eritable
		  d\'efi.
		  
		  Ce rapport se concentre sur la d\'esambigu\"isation des
		  patrons de type et d\'etaille les probl\`emes et leur
		  m\'ethode de r\'esolution, afin de fournir une meilleure
		  plateforme de manipulation de sources.}
}

@TechReport{	  seine.09.seminar,
  author	= {Warren Seine},
  title		= {Type checking in SCOOL with {Transformers} Attribute
		  Grammars},
  titre		= {V\'erification de type en SCOOL avec les Grammaires
		  Attribu\'ees de {Transformers}},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2009,
  abstract	= {SCOOL is a domain-specific language designed to make
		  high-level C++ development easier. Based on a paradigm
		  mixing generic and object-oriented programming, it is able
		  to statically resolve virtual member function calls by
		  discovering the exact type of every object at compile-time.
		  Therefore, the SCOOL-to-C++ translator needs a robust
		  type-checker.
		  
		  Developed in the Stratego/XT environment, SCOOL uses the
		  experience we have had with the Transformers project, a C++
		  program transformation toolkit. Its Attribute grammar
		  system has proven to be efficient for the C++ language. We
		  will test it on the type analysis of SCOOL.},
  resume	= {SCOOL est un langage d\'edi\'e con\c cu pour faciliter le
		  d\'eveloppement C++ de haut niveau. Inspir\'e par un
		  paradigme m\'elangeant programmation g\'en\'erique et
		  orient\'ee objet, il poss\`ede des propri\'et\'es lui
		  permettant de conna\^itre le type exact de tous les objets
		  \`a la compilation, et donc de r\'esoudre les appels de
		  fonctions membres de fa\c con statique. Un v\'erificateur
		  de type est ainsi indispensable au compilateur vers C++.
		  
		  D\'evelopp\'e sous l'environnement Stratego/XT, SCOOL
		  utilise l'exp\'erience que nous poss\'edons avec
		  Transformers, un ensemble d'outils pour la transformation
		  de programmes C++. Son syst\`eme de grammaires attribu\'ees
		  a montr\'e son efficacit\'e sur le C++, nous allons
		  maintenant l'essayer sur l'analyse des types de SCOOL.},
  url		= {http://publications.lrde.epita.fr/200905-Seminar-Seine},
  urllrde	= {200905-Seminar-Seine},
  number	= 0942
}

@TechReport{	  sigoure.06.seminar,
  oldkeys	= {sigoure.06.seminar.xrm},
  author	= {Beno\^it Sigoure},
  title		= {{eX}tended {R}eactive {M}odules},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2006,
  url		= {http://publications.lrde.epita.fr/200607-Seminar-Sigoure},
  urllrde	= {200607-Seminar-Sigoure},
  abstract	= {Reactive Modules is a formal model used to represent
		  synchronous and asynchronous components of a system. PRISM
		  is a widely used probabilistic model-checker. It introduced
		  the PRISM language, highly based on the Reactive Modules
		  formalism. This language quickly reaches its limit when it
		  comes to large models.
		  
		  eXtended Reactive Modules (XRM) is an extension of the
		  PRISM language. It comes with a compiler that translate XRM
		  modules in PRISM modules, thus providing a comprehensive
		  and reliable solution for people willing to write large
		  models.},
  resume	= {Reactive Modules est un mod\`ele formel utilis\'e pour
		  d\'ecrire les \'el\'ements synchrones et asynchrones d'un
		  syst\`eme. PRISM est un outil de model-checking
		  probabiliste. Il a introduit le langage PRISM, grandement
		  bas\'e sur le formalisme de Reactive Modules. Ce langage
		  atteinte vite ses limites lorsqu'il s'agit de d\'ecrire des
		  mod\`eles cons\'equants.
		  
		  eXtended Reactive Modules est une extension du langage
		  PRISM. Il est fournit avec un compilateur qui traduit les
		  modules XRM en modules PRISM, fournissant ainsi une
		  solution fiable et compl\`ete pour les gens ayant besoin de
		  d\'ecrire des syst\`emes cons\'equants.}
}

@TechReport{	  sigoure.07.seminar,
  author	= {Beno\^it Sigoure and Quentin Hocquet},
  title		= {{revCPP} A reversible {C++} preprocessor},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2007,
  url		= {http://publications.lrde.epita.fr/200801-Seminar-Hocquet},
  urllrde	= {200801-Seminar-Hocquet}
}

@TechReport{	  sigoure.08.seminar,
  author	= {Beno\^it Sigoure},
  title		= {Run-Time Concrete-Syntax Program-Transformation in General
		  Purpose Languages},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2008,
  url		= {http://publications.lrde.epita.fr/200801-Seminar-Sigoure-CC}
		  ,
  urllrde	= {200801-Seminar-Sigoure-CC},
  abstract	= {Program transformation in general purpose languages such
		  as \Cxx is tedious because it requires the \acf{ast} of the
		  transformed program to be manipulated in abstract syntax
		  (that is, in the host language, \Cxx here). The code to
		  write is unwieldy and costly to maintain.
		  
		  This object of the seminar is to present the implementation
		  of new concrete syntax program transformation techniques
		  (that is, using directly the language of the transformed
		  program) in a standard \Cxx environment. Our approach uses
		  the parser at run-time to apply dynamic transformation
		  rules. A Tiger compiler will be used to support the
		  presentation.},
  resume	= {La transformation de programmes dans des langages
		  g\'en\'eralistes tels que le \Cxx est fastidieuse car elle
		  n\'ecessite de manipuler l'\acf{ast} du programme transform
		  en syntaxe abstraite (c'est-\`a-dire dans le langage
		  h\^ote, ici le \Cxx). Le code \`a \'ecrire est lourd et
		  co\^uteux maintenir.
		  
		  Le but de ce s\'eminaire est de pr\'esenter la mise en
		  \oe{}uvre de nouvelles techniques de transformation de
		  programmes en syntaxe concr\`ete (c'est-\`a-dire utilisant
		  directement le langage du programme transform\'e) dans un
		  environnement \Cxx standard. Notre approche utilise
		  l'analyseur syntaxique l'ex\'ecution pour appliquer des
		  r\`egles de transformation dynamiques. Un compilateur de
		  Tiger servira de support \`a la pr\'esentation.}
}

@TechReport{	  terrones.05.seminar,
  oldkeys	= {terrones.exp-to-aut.05.seminar},
  author	= { Florent Terrones },
  title		= { From an expression to the original automaton },
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2005,
  urllrde	= {20050622-Seminar-Terrones-DerivedTerms-Slides}
}

@TechReport{	  thivolle.05.seminar,
  oldkeys	= {thivolle.canvas-olena.05.seminar},
  author	= {Damien Thivolle},
  title		= {Canvas in {O}lena},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2005,
  urllrde	= {200506-Seminar-Thivolle}
}

@TechReport{	  van-noppen.07.seminar,
  oldkeys	= {van-noppen.07.seminar.scool},
  author	= {Maxime van Noppen},
  title		= {{SCOOL}: object orientation of a static language},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2007,
  url		= {http://publications.lrde.epita.fr/200706-Seminar-van-Noppen}
		  ,
  urllrde	= {200706-Seminar-van-Noppen}
}

@TechReport{	  van-noppen.08.seminar,
  author	= {Maxime van Noppen},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2008,
  url		= {http://publications.lrde.epita.fr/200806-Seminar-VanNoppen}
		  ,
  urllrde	= {200806-Seminar-VanNoppen},
  titre		= {{SCOOL}: Programmation g\'en\'erique et concepts},
  title		= {{SCOOL}: Concept-Oriented Programming},
  resume	= {\textsc{Scool} est un langage statique orient\'e objet qui
		  a \'et\'e cr\'e\'e afin de pouvoir utiliser toute la
		  puissance du \Cxx statique de mani\`ere plus ais\'ee
		  gr\^ace \`a une syntaxe plus expressive et agr\'eable. Il
		  n'a pas pour but d'\^etre directement compil\'e mais
		  d'\^etre traduit en \Cxx. Cette ann\'ee le travail rev\^et
		  une importance particuli\`ere. En effet, \textsc{Scool} est
		  d\'evelopp\'e en \'etroite collaboration avec l'\'equipe de
		  d\'eveloppement de la biblioth\`eque de traitement d'images
		  \textsc{Milena} de la plate-forme \textsc{Olena} ; l'an
		  pass\'e a \'et\'e pour elle le cadre de grands changements
		  internes. Un des axes majeurs du d\'eveloppement de
		  \textsc{Scool} va donc \^etre de s'adapter aux nouveaux
		  paradigmes et aux nouveaux besoins de la biblioth\`eque. Le
		  second axe essentiel de travail est la poursuite du
		  d\'eveloppement du langage. Cette ann\'ee le travail va
		  \^etre concentr\'e sur la programmation par concepts qui
		  est une approche permettant de formaliser facilement des
		  contraintes sur la programmation g\'en\'erique.},
  abstract	= {\textsc{Scool} is a static object-oriented language. It
		  has been created to help one to take advantage of all the
		  power of static \Cxx thanks to a more expressive syntax. It
		  is not directly compiled but is translated into \Cxx. This
		  year was quite important for the project. Indeed, there are
		  tight links between \textsc{Scool}'s development and the
		  generic image processing library \textsc{Milena} from the
		  \textsc{Olena} platform. As some big changes occured in the
		  library, work needs to be done to adapt the language to its
		  new paradigms and to its new needs. Work also needs to be
		  done to continue the implementation of the different
		  features of \textsc{Scool}. This year the work will be
		  focused on concept-oriented programming. This allows one to
		  easily express constraints on generic programming.}
}

@TechReport{	  van-noppen.09.seminar,
  author	= {Maxime van Noppen},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2008,
  url		= {http://publications.lrde.epita.fr/200901-Seminar-VanNoppen}
		  ,
  urllrde	= {200901-Seminar-VanNoppen},
  titre		= {{SCOOL}: Programmation g\'en\'erique},
  title		= {{SCOOL}: Generic programming},
  resume	= {Le \Cxx est un langage puissant pour \'ecrire des
		  biblioth\`eques g\'en\'eriques et performantes. Cependant,
		  dans certains domaines l'utilisation de l'orient\'e objet
		  usuel peut poser des probl\`emes de performances, comme
		  dans celui des biblioth\`eques de calcul scientifique dans
		  lesquelles de grands ensembles de donn\'ees parcourus par
		  des algorithmes g\'en\'eriques. La solution propos\'ee est
		  la combinaison de la programmation orient\'ee objet
		  classique et de la programmation \emph{statique} qui est en
		  fait de la meta-programmation utilisant intensivement les
		  \emph{templates} du \Cxx. Ceci a l'avantage de remplacer le
		  co\^ut \`a l'\'execution de l'orient\'e objet d\^u \`a la
		  r\'esolution des m\'ethodes virtuelles par un co\^ut `a la
		  compilation. Cependant, cela engendre souvent du code
		  verbeux, difficile \`a \'ecrire et \`a maintenir. Malgr\'e
		  sa puissance, le \Cxx ne poss\`ede pas d'abstractions
		  statiques de haut niveau ce qui encombre la s\'emantique du
		  code avec des d\'etails d'impl\'ementation. Nous vous
		  pr\'esentons \textsc{Scool}, un langage statique fusionnant
		  l'orient\'e objet et la programmation g\'en\'erique dans le
		  but d'\'exploiter toute la puissance du \Cxx statique
		  gr\^ace \`a une syntaxe plus expressive. De plus, toutes
		  les r\'esolutions de m\'ethodes se feront statiquement
		  gr\^ace au fait que le type exact (dynamique) de chaque
		  objet est connu \`a la compilation. Le but de
		  \textsc{Scool} \'etant d'apporter toute la puissance de
		  l'orient\'e objet statique au \Cxx, il ne sera pas
		  directement compil\'e mais traduit en \Cxx correctement
		  format\'e. Le d\'eveloppement du traducteur a soulev\'e les
		  probl\`emes classiques du domaine des DSL comme la
		  strat\'egie de parcours de l'arbre de syntaxe. Nous
		  proposons une solution originale bas\'ee sur la plateforme
		  de transformation de programme Stratego/XT avec des
		  applications \`a Milena, la biblioth\`eque g\'en\'erique et performante de traitement d'image de la plateforme Olena.},
  abstract	= {\Cxx has proved to be a powerful language to write generic
		  and efficient libraries. However using classical \ac{oo}
		  \Cxx may not fulfill the efficiency criterion sought in
		  some domains, e.g. when building scientific libraries,
		  where large data sets have to be processed through generic
		  algorithms. A solution consists in combining the power of
		  \acl{oop} and \emph{static} programming--- which is in fact
		  meta-code expressed thanks to \Cxx template constructs.
		  This has the advantage to replace the \ac{oo} run-time
		  overhead (due to virtual method dispatch) by compile-time
		  computations. However, such an approach relies on code that
		  is verbose, hard to write and to maintain. Though powerful,
		  \Cxx lacks high-level static features, and thus clutters
		  the semantics of static constructs with unrelated code. We
		  present \textsc{Scool}, a static language mixing \ac{oo}
		  and \ac{gp} that has been created to take advantage of all
		  the power of static \Cxx thanks to a more expressive syntax
		  and high-level constructs, without the drawbacks of plain
		  \Cxx. As a full-fledged static OOP language, \textsc{Scool}
		  provides polymorphic methods (i.e., inclusion
		  polymorphism), with the notable difference that every
		  polymorphic call is statically-resolved: the design of
		  \textsc{Scool} is based on the property that the exact
		  (dynamic) type of every object is known at compile-time. As
		  the aim of \textsc{Scool} is to bring all the power of
		  static \acl{oop} to \Cxx, it is not directly compiled but
		  translated into human-readable \Cxx. The development of the
		  translator raised classical problems found in \acp{dsl}
		  like traversal strategies of the abstract syntax tree. We
		  propose an original solution based on the Stratego/XT
		  program transformation framework, and some applications on
		  Milena, a generic and efficient \Cxx library from the Olena
		  image processing platform.}
}

@TechReport{	  vasseur.04.seminar,
  title		= {Semantics driven disambiguation: a comparison of different
		  approaches},
  author	= {Cl\'ement Vasseur},
  institution	= {LRDE},
  year		= 2004,
  urllrde	= {20041201-Seminar-Vasseur-Disambiguation-Report}
}

@TechReport{	  vigouroux.08.seminar,
  author	= {Caroline Vigouroux},
  title		= {Color types in {M}ilena},
  titre		= {Les types de couleur dans Milena},
  institution	= {EPITA Research and Development Laboratory (LRDE)},
  year		= 2008,
  urllrde	= {200807-Seminar-Vigouroux},
  abstract	= {The Olena project provides a generic library for image
		  processing, Milena. We want this library to feature many
		  value types so that the user can always choose the relevant
		  types for his application. For instance, we provide many
		  grey-level types, many color types, etc.
		  
		  This seminar focuses on how we implement color types in
		  Milena. There are different color spaces (RGB, HSI, and so
		  on) and several possible encodings for the same color space
		  (rgb\_3x8, rgb\_f, etc.). One objective of ours is to make
		  things easy for the user. In particular, we want the user
		  to handle color values without being concerned of internal
		  mechanisms. For instance, in conversion formulas, we do not
		  want to see the details of implementation (division by
		  255).},
  resume	= {Le projet Olena fournit une biblioth\`eque g\'en\'erique
		  pour le traitement d'images, Milena. Nous voulons que cette
		  biblioth\`eque procure de nombreux types de valeur tels que
		  l'utilisateur puisse toujours choisir le type adapt\'e pour
		  son application. Par exemple, nous fournissons de nombreux
		  encodages en niveau de gris, de nombreux espaces de
		  couleur, etc.
		  
		  Nous pr\'esentons la mani\`ere dont nous mettons en
		  \oe{}uvre les types de couleurs dans Milena. Il existe
		  diff\'erents espaces de couleur (RGB, HSI, et bien
		  d'autres) et il existe plusieurs encodages possibles pour
		  les m\^emes espaces de couleur (rgb\_3x8, rgb\_f, etc.).
		  Nous voulons rendre les choses plus faciles pour
		  l'utilisateur. Donc, notre objectif est de rendre possible
		  l'utilisation des espaces de couleur sans se soucier des
		  m\'ecanismes internes. Par exemple, dans les formules de
		  conversion, on ne veut pas faire appara\^itre les d\'etails
		  d'impl\'ementation (division par 255).}
}

%%% Local Variables:
%%% fill-column: 76
%%% End:
